{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-24T14:15:41.522842Z","iopub.execute_input":"2026-02-24T14:15:41.523186Z","iopub.status.idle":"2026-02-24T14:15:41.805854Z","shell.execute_reply.started":"2026-02-24T14:15:41.523159Z","shell.execute_reply":"2026-02-24T14:15:41.805134Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"\"\"\"\n# Community Helper Guidebook â€” Low-Resource Helpbook in India\n\n**What this IS:**\nA multimodal, offline-first assistive tool that converts spoken notes and\nimages from community health workers into safe, simplified, multilingual\nguidance packs â€” grounded in real Indian healthcare regulations.\n\n**What this is NOT:** A chatbot, diagnostic engine, or treatment recommender.\n\n---\n\n### Full Pipeline\n```\n  Audio (upload or record live)     Text input         Image upload\n          â†“                               â†“                   â†“\n   Whisper STT                  [skip confirmation]     SigLIP encoding\n   (transcribe mode)\n          â†“\n    Confirmation Layer\n   (helper reviews & corrects transcript â€” skipped for text input)\n          â†“\n   IndicTrans2  â†’  local language to English\n          â†“\n   â˜… RAG Retrieval â˜…\n   (FAISS + sentence-transformers over real Indian MoHFW/NHM documents)\n          â†“  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ image context\n   MedGemma ReAct Agent  (grounded in Indian healthcare regulations)\n     â”œâ”€â”€ get_first_aid_steps(situation)\n     â”œâ”€â”€ check_urgency(symptoms)\n     â”œâ”€â”€ lookup_nearest_facility(region)\n     â””â”€â”€ generate_referral_note(case_summary)\n          â†“\n   Safety Guardrails\n          â†“\n   IndicTrans2  â†’  English to local language\n          â†“\n Helper Guidance Pack\n```\n\n---\n> This system never diagnoses, prescribes, or recommends treatments.\n> Every output instructs escalation to a qualified healthcare professional.\n> Designed for community helpers â€” NOT for patients directly.\n\"\"\"","metadata":{}},{"cell_type":"code","source":"# Install Dependencies\nimport subprocess, sys\n\ndef pip(pkg):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n\npip(\"transformers>=4.40.0\")\npip(\"accelerate>=0.27.0\")\npip(\"bitsandbytes>=0.43.0\")\npip(\"openai-whisper\")\npip(\"pydub\")\npip(\"ffmpeg-python\")\npip(\"Pillow\")\npip(\"gradio>=4.0.0\")\npip(\"sentencepiece\")\npip(\"protobuf\")\npip(\"faiss-cpu\")\npip(\"sentence-transformers\")\npip(\"PyMuPDF\")          # fitz â€” PDF text extraction\npip(\"deep-translator\")\n\nprint(\"All dependencies installed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T15:55:41.477931Z","iopub.execute_input":"2026-02-24T15:55:41.478559Z","iopub.status.idle":"2026-02-24T15:56:26.409731Z","shell.execute_reply.started":"2026-02-24T15:55:41.478532Z","shell.execute_reply":"2026-02-24T15:56:26.409019Z"}},"outputs":[{"name":"stdout","text":"All dependencies installed.\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# Imports and Configs\n\nimport os\nimport re\nimport json\nimport warnings\nimport tempfile\nimport urllib.request\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport torch\nimport numpy as np\nimport faiss\nimport fitz                          # PyMuPDF\nimport whisper\nfrom PIL import Image, ImageDraw\nfrom pydub import AudioSegment\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    AutoProcessor,\n    AutoModel,\n    BitsAndBytesConfig,\n)\n\nwarnings.filterwarnings(\"ignore\")\n\n# â”€â”€ Device â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDTYPE    = torch.bfloat16 if torch.cuda.is_available() else torch.float32\nUSE_4BIT = torch.cuda.is_available()\n\n# â”€â”€ Model IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nMEDGEMMA_ID   = \"google/medgemma-4b-it\"\nMEDSIG_ID     = \"google/siglip-so400m-patch14-384\"\nWHISPER_SIZE  = \"medium\"\nEMBEDDER_ID   = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n# â”€â”€ Supported languages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSUPPORTED_LANGUAGES = {\n    \"Hindi\"   : \"hi\",\n    \"Tamil\"   : \"ta\",\n    \"Bengali\" : \"bn\",\n    \"Telugu\"  : \"te\",\n    \"Marathi\" : \"mr\",\n    \"Gujarati\": \"gu\",\n    \"Kannada\" : \"kn\",\n    \"English\" : \"en\",\n}\n\n# â”€â”€ Safety disclaimer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDISCLAIMER = (\n    \"\\n\\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\n\"\n    \" IMPORTANT NOTICE\\n\"\n    \"This is assistive guidance for community health workers only.\\n\"\n    \"It does NOT replace clinical advice or medical examination.\\n\"\n    \"Always escalate to a qualified healthcare professional.\\n\"\n    \"Emergency Ambulance: Call 108 (free, 24/7)\\n\"\n    \"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\"\n)\n\nprint(f\"Config ready\")\nprint(f\"   Device   : {DEVICE.upper()}\")\nprint(f\"   Whisper  : {WHISPER_SIZE}\")\nprint(f\"   MedGemma : {MEDGEMMA_ID}\")\nprint(f\"   Embedder : {EMBEDDER_ID}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T14:16:38.196267Z","iopub.execute_input":"2026-02-24T14:16:38.196467Z","iopub.status.idle":"2026-02-24T14:16:59.399967Z","shell.execute_reply.started":"2026-02-24T14:16:38.196447Z","shell.execute_reply":"2026-02-24T14:16:59.399232Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n  elif re.match('(flt)p?( \\(default\\))?$', token):\n/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n  elif re.match('(dbl)p?( \\(default\\))?$', token):\n","output_type":"stream"},{"name":"stdout","text":"Config ready\n   Device   : CUDA\n   Whisper  : medium\n   MedGemma : google/medgemma-4b-it\n   Embedder : sentence-transformers/all-MiniLM-L6-v2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nsecrets  = UserSecretsClient()\nhf_token = secrets.get_secret(\"HF_TOKEN\")\nlogin(token=hf_token)\nprint(\"HuggingFace login successful.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T14:16:59.401719Z","iopub.execute_input":"2026-02-24T14:16:59.402158Z","iopub.status.idle":"2026-02-24T14:16:59.643035Z","shell.execute_reply.started":"2026-02-24T14:16:59.402129Z","shell.execute_reply":"2026-02-24T14:16:59.642298Z"}},"outputs":[{"name":"stdout","text":"HuggingFace login successful.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Load MedGemma 4B-IT (4-bit quantized)\nprint(\"Loading MedGemma 4B â€” may take 3â€“5 minutes on first run...\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit              = True,\n    bnb_4bit_quant_type       = \"nf4\",\n    bnb_4bit_compute_dtype    = DTYPE,\n    bnb_4bit_use_double_quant = True,\n) if USE_4BIT else None\n\ntokenizer = AutoTokenizer.from_pretrained(\n    MEDGEMMA_ID, trust_remote_code=True\n)\n\nmedgemma = AutoModelForCausalLM.from_pretrained(\n    MEDGEMMA_ID,\n    quantization_config = bnb_config,\n    device_map          = \"auto\" if torch.cuda.is_available() else None,\n    torch_dtype         = DTYPE,\n    trust_remote_code   = True,\n)\nmedgemma.eval()\nprint(\"MedGemma 4B loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T14:16:59.644073Z","iopub.execute_input":"2026-02-24T14:16:59.644492Z","iopub.status.idle":"2026-02-24T14:17:39.440828Z","shell.execute_reply.started":"2026-02-24T14:16:59.644462Z","shell.execute_reply":"2026-02-24T14:17:39.440081Z"}},"outputs":[{"name":"stdout","text":"Loading MedGemma 4B â€” may take 3â€“5 minutes on first run...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a340d7dbfa69462bac55aee753bd395e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7029ddfd07e74819ad70a369e7619c1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0917542fa940477eb83f6c73f1d4ff61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24b1273d9a114859934a019c01aacb37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddaae011ca234710813b8cad3b587c7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48a0849214684236b9d0771681d5430f"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a8698f0186c409cbcc47be261eb2d05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (incomplete total...): 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8531735f9a76479db0286181c0141002"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c576a43ef34e43c997dc15a08018c55e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9314374ea3840439349e9572a5f512b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7055b3da2b3b4d72b9c03b1f79dae361"}},"metadata":{}},{"name":"stdout","text":"MedGemma 4B loaded.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Load SigLIP (MedSigLIP proxy)\n# NOTE: SigLIP is used as a capable zero-shot proxy for MedSigLIP.\n# In production: replace with a fine-tuned MedSigLIP checkpoint\n\nprint(\" Loading SigLIP image encoder (MedSigLIP proxy)...\")\n\nsiglip_processor = AutoProcessor.from_pretrained(MEDSIG_ID)\nsiglip_model     = AutoModel.from_pretrained(MEDSIG_ID, torch_dtype=DTYPE)\nsiglip_model     = siglip_model.to(DEVICE).eval()\n\nprint(\"SigLIP loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T14:17:39.441826Z","iopub.execute_input":"2026-02-24T14:17:39.442606Z","iopub.status.idle":"2026-02-24T14:17:52.460440Z","shell.execute_reply.started":"2026-02-24T14:17:39.442576Z","shell.execute_reply":"2026-02-24T14:17:52.459660Z"}},"outputs":[{"name":"stdout","text":" Loading SigLIP image encoder (MedSigLIP proxy)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/368 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0158b9408a734009840a30f1e20db760"}},"metadata":{}},{"name":"stderr","text":"The image processor of type `SiglipImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/576 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc3ea8e4176b4991971458fb8a12d755"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/711 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecb3dddf03bd4dc4b40ce1cfae691cb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e9d4b23e728418d97d70206479d61f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/409 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"416541482430409fb97a299e84fcacda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05d5ced1db1b4cbf8a8da3c416869fe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.51G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"719f851146164395bcc7b5ecd5fc8f11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/888 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1692643339d24b79a4db1c7096e70f65"}},"metadata":{}},{"name":"stdout","text":"SigLIP loaded.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Load Whisper STT\nprint(f\"Loading Whisper ({WHISPER_SIZE})...\")\nwhisper_model = whisper.load_model(WHISPER_SIZE, device=DEVICE)\nprint(\"Whisper loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T14:17:52.461538Z","iopub.execute_input":"2026-02-24T14:17:52.461859Z","iopub.status.idle":"2026-02-24T14:18:24.982583Z","shell.execute_reply.started":"2026-02-24T14:17:52.461825Z","shell.execute_reply":"2026-02-24T14:18:24.981943Z"}},"outputs":[{"name":"stdout","text":"Loading Whisper (medium)...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.42G/1.42G [00:19<00:00, 80.4MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"Whisper loaded.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Load Sentence Embedder (for RAG)\nprint(f\"Loading sentence embedder ({EMBEDDER_ID})...\")\nembedder = SentenceTransformer(EMBEDDER_ID)\nprint(\"Sentence embedder loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T14:18:24.983365Z","iopub.execute_input":"2026-02-24T14:18:24.983562Z","iopub.status.idle":"2026-02-24T14:18:27.785031Z","shell.execute_reply.started":"2026-02-24T14:18:24.983544Z","shell.execute_reply":"2026-02-24T14:18:27.784194Z"}},"outputs":[{"name":"stdout","text":"Loading sentence embedder (sentence-transformers/all-MiniLM-L6-v2)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a20d1b10a1454e3db52a6014eb528c2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4da41cc638944569fe1cbd25a2116fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42a29daf8c6c42fab27522e4f894dfe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41c7d64677844195a8d66ae2d683faa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bffdeb506c7d435ea24ed52b7ad78cc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9664beb36a694fb4a2b7f023dee89c36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98f89258639941c487decc96179f259c"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\nKey                     | Status     |  | \n------------------------+------------+--+-\nembeddings.position_ids | UNEXPECTED |  | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"844622ba09104445a5674424593827ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04fbe3ec7ca343609f8db952a3a5ff3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ec4039bbf349e5aa52571ea082f8a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce24de6227814ce5904747ed4a0df502"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4bc851348bd4d1496427e9007f42755"}},"metadata":{}},{"name":"stdout","text":"Sentence embedder loaded.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# RAG â€” Indian Healthcare Document Knowledge Base\n# RAG KNOWLEDGE BASE                                                                                                       \n# Sources:                                                    \n# 1. National Health Policy 2017 (MoHFW)                     \n# 2. IPHS Guidelines for CHC/PHC 2022 (NHM)                  \n# 3. NHM Community Health Worker Guidelines                   \n# 4. Key excerpts: ASHA Training Module + AB PM-JAY           \n#     (embedded directly â€” guaranteed availability)            \n                                                           \n# Note: In production, LangChain + ChromaDB can replace       \n# this FAISS implementation for easier document management,   \n# persistent storage, and metadata filtering.                 \n\n\n# Real document URLs (public government sources)\n\nDOCUMENT_URLS = {\n    \"national_health_policy_2017\": (\n        \"https://www.mohfw.gov.in/sites/default/files/9147562941489753121.pdf\"\n    ),\n    \"iphs_guidelines_chc_phc\": (\n        \"https://nhm.gov.in/images/pdf/guidelines/iphs/iphs-revised-guidlines-2012/\"\n        \"primay-health-centres.pdf\"\n    ),\n    \"nhm_community_health\": (\n        \"https://nhm.gov.in/images/pdf/communitisation/resource-material/\"\n        \"CHW-Guidelines.pdf\"\n    ),\n}\n\n# â”€â”€ Key excerpts embedded directly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# These cover ASHA protocols and Ayushman Bharat â€” included as text\n# to guarantee availability regardless of PDF accessibility.\n\nEMBEDDED_EXCERPTS = [\n    # â”€â”€ ASHA Worker Protocols (MoHFW Training Module) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    {\n        \"source\" : \"ASHA Training Module â€” MoHFW\",\n        \"text\"   : (\n            \"ASHA (Accredited Social Health Activist) is a trained female community \"\n            \"health activist. She is the first port of call for any health related \"\n            \"demands of deprived sections of the population, especially women and \"\n            \"children who find it difficult to access health services. \"\n            \"ASHA is expected to be a resident of the village, married, widowed or \"\n            \"divorced, preferably in the age group of 25 to 45 years, with formal \"\n            \"education up to 8th class. \"\n            \"ASHA acts as an interface between the community and the public health system.\"\n        ),\n    },\n    {\n        \"source\" : \"ASHA Training Module â€” MoHFW\",\n        \"text\"   : (\n            \"ASHA's role is not to diagnose or treat illness. Her role is to counsel \"\n            \"community members on health, nutrition, and hygiene. She mobilises the \"\n            \"community for health services, facilitates access to improved healthcare \"\n            \"at the community level, and acts as the first contact for the community \"\n            \"with the health system. ASHA is a link worker, not a healthcare provider. \"\n            \"She should refer serious cases to the Sub-Centre, PHC, or CHC immediately.\"\n        ),\n    },\n    {\n        \"source\" : \"ASHA Training Module â€” MoHFW\",\n        \"text\"   : (\n            \"Danger signs requiring immediate referral by ASHA: \"\n            \"High fever with convulsions, unconsciousness, difficulty breathing, \"\n            \"severe bleeding, snake bite, poisoning, chest pain, prolonged labour, \"\n            \"newborn not breathing, severe malnutrition with complications. \"\n            \"In all such cases ASHA must call 108 ambulance and accompany patient \"\n            \"to the nearest PHC or District Hospital.\"\n        ),\n    },\n    {\n        \"source\" : \"ASHA Training Module â€” MoHFW\",\n        \"text\"   : (\n            \"First aid role of ASHA: ASHA is trained only in basic first aid. \"\n            \"This includes keeping the patient calm, placing the patient in a safe \"\n            \"position, preventing further harm, and arranging for immediate referral. \"\n            \"ASHA must not attempt to diagnose illness, prescribe medicines, or \"\n            \"perform clinical procedures. If in doubt, always refer to a health worker.\"\n        ),\n    },\n\n    # â”€â”€ PHC/CHC Referral Protocols (NHM IPHS Guidelines) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    {\n        \"source\" : \"IPHS Guidelines for PHC â€” NHM 2022\",\n        \"text\"   : (\n            \"Primary Health Centre (PHC) is the first contact point between village \"\n            \"community and the Medical Officer. The PHC covers a population of 30,000 \"\n            \"in plain areas and 20,000 in hilly or tribal areas. \"\n            \"PHC provides curative, preventive, promotive and rehabilitative primary \"\n            \"healthcare services to the rural population. \"\n            \"Referral from PHC to CHC or District Hospital is mandatory for cases \"\n            \"requiring specialist care, surgery, or intensive management.\"\n        ),\n    },\n    {\n        \"source\" : \"IPHS Guidelines for CHC â€” NHM 2022\",\n        \"text\"   : (\n            \"Community Health Centre (CHC) serves as a referral centre for PHCs. \"\n            \"CHC covers a population of 1,20,000 in plain areas. \"\n            \"It provides specialist services in Medicine, Surgery, Obstetrics, \"\n            \"Gynaecology, and Paediatrics. \"\n            \"Cases beyond CHC capacity are referred to District Hospital or \"\n            \"Medical College Hospital. \"\n            \"Emergency services at CHC are available 24 hours, 7 days a week.\"\n        ),\n    },\n    {\n        \"source\" : \"IPHS Guidelines â€” NHM 2022\",\n        \"text\"   : (\n            \"Referral protocol: Community health worker identifies case â†’ \"\n            \"Provides immediate first aid â†’ Calls 108 ambulance if urgent â†’ \"\n            \"Refers to Sub-Centre or PHC with written referral note â†’ \"\n            \"PHC assesses and refers to CHC if needed â†’ \"\n            \"CHC refers to District Hospital for specialist care. \"\n            \"All referrals must be accompanied by a written note describing \"\n            \"the patient's condition and actions taken.\"\n        ),\n    },\n\n    # â”€â”€ National Health Policy 2017 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    {\n        \"source\" : \"National Health Policy 2017 â€” MoHFW\",\n        \"text\"   : (\n            \"The National Health Policy 2017 aims to achieve the highest possible \"\n            \"level of good health and well-being through a preventive and promotive \"\n            \"healthcare orientation in all developmental policies. \"\n            \"It advocates reaching healthcare to the doorstep of citizens in rural \"\n            \"and underserved areas through community health workers. \"\n            \"The policy emphasises that community health workers should be empowered \"\n            \"with information and tools â€” not burdened with clinical responsibilities \"\n            \"beyond their training.\"\n        ),\n    },\n    {\n        \"source\" : \"National Health Policy 2017 â€” MoHFW\",\n        \"text\"   : (\n            \"Digital health is recognised in the National Health Policy 2017 as a \"\n            \"key enabler for health system strengthening. \"\n            \"Technology-enabled tools for community health workers should support \"\n            \"record-keeping, referral, and communication with higher health facilities. \"\n            \"Such tools must be accessible in local languages and usable by workers \"\n            \"with limited literacy. Privacy and confidentiality of patient data \"\n            \"must be maintained at all times.\"\n        ),\n    },\n\n    # â”€â”€ Ayushman Bharat PM-JAY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    {\n        \"source\" : \"Ayushman Bharat PM-JAY â€” NHA Operational Guidelines\",\n        \"text\"   : (\n            \"Ayushman Bharat Pradhan Mantri Jan Arogya Yojana (PM-JAY) provides \"\n            \"health cover of Rs. 5 lakhs per family per year for secondary and \"\n            \"tertiary hospitalisation. \"\n            \"Beneficiaries include families identified as poor and vulnerable as per \"\n            \"SECC database. The scheme covers over 10.74 crore poor and vulnerable \"\n            \"families â€” approximately 50 crore beneficiaries. \"\n            \"All public hospitals and empanelled private hospitals are covered. \"\n            \"Community health workers should inform eligible families about PM-JAY \"\n            \"benefits and assist them in accessing scheme cards.\"\n        ),\n    },\n    {\n        \"source\" : \"Ayushman Bharat PM-JAY â€” NHA Operational Guidelines\",\n        \"text\"   : (\n            \"Under Ayushman Bharat, Health and Wellness Centres (HWCs) are the \"\n            \"first level of care for the community. \"\n            \"HWCs provide comprehensive primary healthcare including maternal health, \"\n            \"child health, communicable and non-communicable diseases, and basic \"\n            \"emergency services. \"\n            \"Community health workers must refer patients to the nearest HWC or PHC \"\n            \"as the first point of formal medical contact. \"\n            \"Emergency referrals should always use the 108 ambulance service.\"\n        ),\n    },\n\n    # â”€â”€ NHM Community Health Worker Guidelines â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    {\n        \"source\" : \"NHM Community Health Worker Guidelines\",\n        \"text\"   : (\n            \"Community health workers must follow these core principles: \"\n            \"1. Do no harm â€” never attempt procedures beyond your training. \"\n            \"2. Refer early â€” do not delay referral for serious conditions. \"\n            \"3. Communicate clearly â€” use simple language the family understands. \"\n            \"4. Document everything â€” record all observations and actions taken. \"\n            \"5. Maintain confidentiality â€” do not share patient information publicly. \"\n            \"6. Follow up â€” check on referred patients to ensure they received care.\"\n        ),\n    },\n    {\n        \"source\" : \"NHM Community Health Worker Guidelines\",\n        \"text\"   : (\n            \"Community health workers should NEVER: \"\n            \"Diagnose any illness or condition. \"\n            \"Prescribe or dispense medications beyond the approved drug kit. \"\n            \"Perform injections unless specifically trained and authorised. \"\n            \"Conduct deliveries without trained supervision. \"\n            \"Give dietary supplements without proper assessment. \"\n            \"Make promises about recovery or prognosis. \"\n            \"Delay referral to attempt home treatment for serious conditions.\"\n        ),\n    },\n]\n\n\ndef extract_pdf_text(pdf_path: str, max_chars: int = 50000) -> str:\n    \"\"\"Extracts text from a PDF file using PyMuPDF (fitz).\"\"\"\n    doc  = fitz.open(pdf_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n        if len(text) > max_chars:\n            break\n    return text[:max_chars]\n\n\ndef chunk_text(text: str, chunk_size: int = 500, overlap: int = 100) -> list[str]:\n    \"\"\"\n    Splits text into overlapping chunks for embedding.\n    Overlap ensures context is preserved across chunk boundaries.\n    \"\"\"\n    words  = text.split()\n    chunks = []\n    start  = 0\n    while start < len(words):\n        end   = min(start + chunk_size, len(words))\n        chunk = \" \".join(words[start:end])\n        if len(chunk.strip()) > 50:   # skip very short chunks\n            chunks.append(chunk)\n        start += chunk_size - overlap\n    return chunks\n\n\ndef download_pdf(url: str, name: str) -> str | None:\n    \"\"\"Downloads a PDF to /tmp. Returns path or None if failed.\"\"\"\n    path = f\"/tmp/{name}.pdf\"\n    if os.path.exists(path):\n        print(f\"   âœ“ {name} already cached.\")\n        return path\n    try:\n        print(f\"   â†³ Downloading {name}...\")\n        urllib.request.urlretrieve(url, path)\n        return path\n    except Exception as e:\n        print(f\"   âŒ TRANSLATION FAILED: {e}\")\n        return text, \"passthrough (translation failed)\"\n\n\ndef build_rag_index() -> tuple[faiss.Index, list[dict]]:\n    \"\"\"\n    Builds FAISS vector index from:\n    1. Real Indian government PDFs (downloaded at runtime)\n    2. Embedded excerpts (ASHA protocols, PM-JAY, NHM guidelines)\n\n    Returns (faiss_index, document_store).\n    document_store: list of {text, source} dicts aligned with FAISS index.\n\n    NOTE: In production, LangChain + ChromaDB can replace this FAISS\n    implementation for easier document management, persistent vector\n    storage, metadata filtering, and hybrid search.\n    \"\"\"\n    all_chunks = []\n\n    # â”€â”€ Step 1: Download and extract real PDFs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    print(\"\\nBuilding RAG knowledge base from Indian healthcare documents...\")\n    for name, url in DOCUMENT_URLS.items():\n        pdf_path = download_pdf(url, name)\n        if pdf_path:\n            try:\n                raw_text = extract_pdf_text(pdf_path)\n                chunks   = chunk_text(raw_text)\n                for chunk in chunks:\n                    all_chunks.append({\"text\": chunk, \"source\": name})\n                print(f\"   âœ“ {name}: {len(chunks)} chunks extracted.\")\n            except Exception as e:\n                print(f\" Could not extract text from {name}: {e}\")\n\n    # â”€â”€ Step 2: Add embedded excerpts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    print(f\"   â†³ Adding {len(EMBEDDED_EXCERPTS)} embedded regulatory excerpts...\")\n    for excerpt in EMBEDDED_EXCERPTS:\n        chunks = chunk_text(excerpt[\"text\"])\n        for chunk in chunks:\n            all_chunks.append({\"text\": chunk, \"source\": excerpt[\"source\"]})\n\n    print(f\"\\n   Total chunks in knowledge base: {len(all_chunks)}\")\n\n    # â”€â”€ Step 3: Embed all chunks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    print(\"   â†³ Embedding chunks...\")\n    texts      = [c[\"text\"] for c in all_chunks]\n    embeddings = embedder.encode(texts, show_progress_bar=True, batch_size=64)\n    embeddings = np.array(embeddings, dtype=np.float32)\n\n    # â”€â”€ Step 4: Build FAISS index â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    dim   = embeddings.shape[1]\n    index = faiss.IndexFlatL2(dim)\n    index.add(embeddings)\n\n    print(f\"RAG index built: {index.ntotal} vectors indexed.\")\n    return index, all_chunks\n\n\ndef retrieve_context(\n    query      : str,\n    index      : faiss.Index,\n    doc_store  : list[dict],\n    top_k      : int = 4,\n) -> str:\n    \"\"\"\n    Retrieves top-k most relevant chunks from the knowledge base\n    for a given query. Returns formatted context string for MedGemma.\n    \"\"\"\n    query_vec = embedder.encode([query], show_progress_bar=False)\n    query_vec = np.array(query_vec, dtype=np.float32)\n\n    distances, indices = index.search(query_vec, top_k)\n\n    context_parts = []\n    for rank, idx in enumerate(indices[0]):\n        if idx < len(doc_store):\n            chunk  = doc_store[idx]\n            source = chunk[\"source\"]\n            text   = chunk[\"text\"]\n            context_parts.append(\n                f\"[Source {rank+1}: {source}]\\n{text}\"\n            )\n\n    return \"\\n\\n\".join(context_parts)\n\n\n# â”€â”€ Build the index at startup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nrag_index, rag_doc_store = build_rag_index()\n\nprint(\"\\n RAG module ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T14:52:43.649276Z","iopub.execute_input":"2026-02-24T14:52:43.650018Z","iopub.status.idle":"2026-02-24T14:52:43.990927Z","shell.execute_reply.started":"2026-02-24T14:52:43.649992Z","shell.execute_reply":"2026-02-24T14:52:43.990083Z"}},"outputs":[{"name":"stdout","text":"\nBuilding RAG knowledge base from Indian healthcare documents...\n   âœ“ national_health_policy_2017 already cached.\n   âœ“ national_health_policy_2017: 18 chunks extracted.\n   âœ“ iphs_guidelines_chc_phc already cached.\n   âœ“ iphs_guidelines_chc_phc: 18 chunks extracted.\n   âœ“ nhm_community_health already cached.\n   âœ“ nhm_community_health: 1 chunks extracted.\n   â†³ Adding 13 embedded regulatory excerpts...\n\n   Total chunks in knowledge base: 50\n   â†³ Embedding chunks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fda0524f82447e5a2075515bd1f202e"}},"metadata":{}},{"name":"stdout","text":"RAG index built: 50 vectors indexed.\n\n RAG module ready.\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# Module A â€” Whisper STT + pydub Audio Conversion\n\nSUPPORTED_AUDIO_FORMATS = [\".wav\", \".mp3\", \".m4a\", \".ogg\", \".flac\", \".webm\"]\n\n\ndef convert_audio(input_path: str) -> str:\n    \"\"\"\n    Converts any audio format to 16kHz mono WAV using pydub + ffmpeg.\n    Whisper performs best on 16kHz mono WAV.\n    Returns path to converted temp file.\n    \"\"\"\n    ext = Path(input_path).suffix.lower()\n    if ext not in SUPPORTED_AUDIO_FORMATS:\n        raise ValueError(\n            f\"Unsupported format: {ext}. Supported: {SUPPORTED_AUDIO_FORMATS}\"\n        )\n\n    audio = AudioSegment.from_file(input_path)\n    audio = audio.set_channels(1).set_frame_rate(16000)\n\n    out_path = tempfile.mktemp(suffix=\".wav\")\n    audio.export(out_path, format=\"wav\")\n    return out_path\n\n\ndef transcribe_audio(audio_path: str, language: str = \"hi\") -> tuple[str, str]:\n    \"\"\"\n    Transcribes audio using Whisper in TRANSCRIBE mode.\n    Output stays in source language â€” IndicTrans2 handles translation.\n    Returns (transcript, detected_language).\n    \"\"\"\n    converted = convert_audio(audio_path)\n\n    result = whisper_model.transcribe(\n        converted,\n        language = language,\n        task     = \"transcribe\",\n        fp16     = torch.cuda.is_available(),\n    )\n\n    transcript        = result[\"text\"].strip()\n    detected_language = result.get(\"language\", language)\n\n    try:\n        os.remove(converted)\n    except Exception:\n        pass\n\n    return transcript, detected_language\n\n\nprint(\"STT module ready (Whisper + pydub).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T14:18:37.657756Z","iopub.execute_input":"2026-02-24T14:18:37.657992Z","iopub.status.idle":"2026-02-24T14:18:37.665416Z","shell.execute_reply.started":"2026-02-24T14:18:37.657971Z","shell.execute_reply":"2026-02-24T14:18:37.664636Z"}},"outputs":[{"name":"stdout","text":"STT module ready (Whisper + pydub).\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Module B â€” Confirmation Layer\n\ndef prepare_confirmation(\n    transcript : str,\n    input_mode : str,\n    detected   : str = \"\",\n) -> dict:\n    \"\"\"\n    Returns confirmation metadata.\n    - Text input  â†’ skip = True\n    - Audio input â†’ skip = False (helper reviews editable transcript)\n    \"\"\"\n    if input_mode == \"text\":\n        return {\n            \"skip\"    : True,\n            \"message\" : \"Text input â€” confirmation step skipped.\",\n        }\n    return {\n        \"skip\"    : False,\n        \"message\" : (\n            f\"ðŸŽ™ï¸ Whisper transcribed the following \"\n            f\"(detected language: {detected}).\\n\"\n            \"Please review and correct any errors before generating guidance:\"\n        ),\n    }\n\n\nprint(\"Confirmation layer ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T14:18:37.666471Z","iopub.execute_input":"2026-02-24T14:18:37.666740Z","iopub.status.idle":"2026-02-24T14:18:37.679982Z","shell.execute_reply.started":"2026-02-24T14:18:37.666711Z","shell.execute_reply":"2026-02-24T14:18:37.679422Z"}},"outputs":[{"name":"stdout","text":"Confirmation layer ready.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Module C â€” IndicTrans2 Translation (mock, swap-in ready)\n\nfrom deep_translator import GoogleTranslator\n\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  PRODUCTION SWAP-IN                                          â•‘\n# â•‘  Replace GoogleTranslator calls with IndicTrans2:            â•‘\n# â•‘                                                              â•‘\n# â•‘  from IndicTrans2 import Translator                          â•‘\n# â•‘  t = Translator(src_lang=src_code, tgt_lang=\"en\")            â•‘\n# â•‘  return t.translate(text), \"IndicTrans2\"                     â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef translate_to_english(text: str, source_lang: str = \"Hindi\") -> tuple[str, str]:\n    \"\"\"\n    Translates local-language text to English using Google Translate.\n    Production swap-in: replace with IndicTrans2 for higher accuracy\n    on Indian languages and offline capability.\n    \"\"\"\n    if source_lang == \"English\":\n        return text, \"passthrough\"\n\n    lang_code = SUPPORTED_LANGUAGES.get(source_lang, \"hi\")\n\n    try:\n        # Chunk to stay under Google Translate 5000 char limit\n        chunks     = [text[i:i+4500] for i in range(0, len(text), 4500)]\n        translated = \" \".join(\n            GoogleTranslator(source=lang_code, target=\"en\").translate(chunk)\n            for chunk in chunks\n        )\n        return translated, \"GoogleTranslate (demo)\"\n\n    except Exception as e:\n        print(f\" Traslation to English failed: {e} â€” using original text\")\n        return text, \"passthrough (failed)\"\n\n\ndef translate_from_english(text: str, target_lang: str = \"Hindi\") -> tuple[str, str]:\n    \"\"\"\n    Translates English guidance back to local language using Google Translate.\n    Production swap-in: replace with IndicTrans2 for higher accuracy\n    on Indian languages and offline capability.\n    \"\"\"\n    if target_lang == \"English\":\n        return text, \"passthrough\"\n\n    lang_code = SUPPORTED_LANGUAGES.get(target_lang, \"hi\")\n\n    try:\n        # Chunk to stay under Google Translate 5000 char limit\n        chunks     = [text[i:i+4500] for i in range(0, len(text), 4500)]\n        translated = \" \".join(\n            GoogleTranslator(source=\"en\", target=lang_code).translate(chunk)\n            for chunk in chunks\n        )\n        return translated, \"GoogleTranslate (demo)\"\n\n    except Exception as e:\n        print(f\"   Translation from English failed: {e} â€” returning English\")\n        return text, \"passthrough (failed)\"\n\n\nprint(\"Translation module ready (Google Translate â€” IndicTrans2 swap-in ready).\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T15:57:11.142603Z","iopub.execute_input":"2026-02-24T15:57:11.143172Z","iopub.status.idle":"2026-02-24T15:57:11.152221Z","shell.execute_reply.started":"2026-02-24T15:57:11.143144Z","shell.execute_reply":"2026-02-24T15:57:11.151368Z"}},"outputs":[{"name":"stdout","text":"Translation module ready (Google Translate â€” IndicTrans2 swap-in ready).\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# Module D â€” SigLIP Image Encoding\n\nMEDICAL_IMAGE_LABELS = [\n    \"chest X-ray\",\n    \"skin lesion or open wound\",\n    \"eye or retina photograph\",\n    \"handwritten medical prescription\",\n    \"ultrasound scan image\",\n    \"ECG or EKG tracing\",\n    \"burn injury on skin\",\n    \"skin rash or dermatological condition\",\n    \"fracture or bone X-ray\",\n    \"general medical document or lab report\",\n]\n\n\ndef encode_image(image: Image.Image) -> dict:\n    \"\"\"\n    Zero-shot medical image classification using SigLIP.\n    Returns image type, confidence, and a context string for MedGemma.\n    Does NOT provide diagnosis â€” supporting context only.\n    \"\"\"\n    inputs = siglip_processor(\n        text           = MEDICAL_IMAGE_LABELS,\n        images         = image,\n        return_tensors = \"pt\",\n        padding        = True,\n    ).to(DEVICE)\n\n    with torch.no_grad():\n        outputs = siglip_model(**inputs)\n        probs   = torch.softmax(\n            outputs.logits_per_image[0], dim=-1\n        ).float().cpu().numpy()\n\n    top_idx   = int(np.argmax(probs))\n    top_label = MEDICAL_IMAGE_LABELS[top_idx]\n    top_score = float(probs[top_idx])\n\n    return {\n        \"image_type\"      : top_label,\n        \"confidence\"      : round(top_score, 3),\n        \"all_labels\"      : {\n            label: round(float(p), 3)\n            for label, p in zip(MEDICAL_IMAGE_LABELS, probs)\n        },\n        \"context_for_llm\" : (\n            f\"The community helper has uploaded an image. \"\n            f\"It appears to show: {top_label} \"\n            f\"(confidence: {top_score:.0%}). \"\n            f\"Use this as supporting context only. \"\n            f\"Do NOT provide a diagnosis based on this image.\"\n        ),\n    }\n\n\nprint(\"Image encoding module ready (SigLIP / MedSigLIP proxy).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T14:18:37.980254Z","iopub.execute_input":"2026-02-24T14:18:37.980855Z","iopub.status.idle":"2026-02-24T14:18:37.987839Z","shell.execute_reply.started":"2026-02-24T14:18:37.980832Z","shell.execute_reply":"2026-02-24T14:18:37.987137Z"}},"outputs":[{"name":"stdout","text":"Image encoding module ready (SigLIP / MedSigLIP proxy).\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Module E â€” Agentic Tool Definitions\n\n\n# TOOL 1: get_first_aid_steps\n# MedGemma generates bare-hands first aid steps.\n# Strict prompt boundary: no medication, no clinical procedures.\n# First aid definition: immediate care before professional help arrives.\n\n\nFIRST_AID_SYSTEM_PROMPT = \"\"\"You are a first aid assistant for community health workers in rural India.\n\nGenerate ONLY first aid steps for the given situation.\nSteps must be bare-hands only â€” no medication, no equipment, no clinical procedures.\n\nRespond in this EXACT format and nothing else. Do not repeat. Stop after the last step:\n\nDO THIS NOW:\n1. [one specific step for this situation]\n2. [one specific step for this situation]\n3. [one specific step for this situation]\n\nDO NOT DO:\n1. [one thing to avoid for this situation]\n2. [one thing to avoid for this situation]\"\"\"\n\ndef call_medgemma_prompt(FIRST_AID_SYSTEM_PROMPT, max_new_tokens=500):\n    inputs = tokenizer(\n        FIRST_AID_SYSTEM_PROMPT,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True\n    )\n\n    input_ids = inputs[\"input_ids\"].to(medgemma.device)\n    attention_mask = inputs.get(\"attention_mask\", None)\n    if attention_mask is not None:\n        attention_mask = attention_mask.to(medgemma.device)\n\n    with torch.no_grad():\n        output_ids = medgemma.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=max_new_tokens,\n            do_sample=False\n        )\n\n    return tokenizer.decode(\n        output_ids[0][input_ids.shape[-1]:],\n        skip_special_tokens=True\n    )\n\ndef get_first_aid_steps(situation: str) -> dict:\n    messages = [\n        {\"role\": \"system\", \"content\": FIRST_AID_SYSTEM_PROMPT},\n        {\"role\": \"user\",   \"content\": f\"Situation: {situation}\"},\n    ]\n\n    input_ids = tokenizer.apply_chat_template(\n        messages,\n        tokenize              = True,\n        add_generation_prompt = True,\n        return_tensors        = \"pt\",\n    )\n\n    if hasattr(input_ids, \"input_ids\"):\n        input_ids = input_ids.input_ids\n    input_ids = input_ids.to(DEVICE)\n\n    with torch.no_grad():\n        output_ids = medgemma.generate(\n            input_ids,\n            max_new_tokens = 150,        # strict limit â€” steps only\n            temperature    = 0.1,        # low temp â€” no creativity needed\n            do_sample      = True,\n            pad_token_id   = tokenizer.eos_token_id,\n            eos_token_id   = tokenizer.eos_token_id,\n        )\n\n    new_tokens = output_ids[0][input_ids.shape[-1]:]\n    steps      = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n\n    # Keep only the first aid block â€” stop at any repetition\n    lines        = steps.split(\"\\n\")\n    clean_lines  = []\n    seen         = set()\n    for line in lines:\n        line_stripped = line.strip()\n        if line_stripped and line_stripped not in seen:\n            seen.add(line_stripped)\n            clean_lines.append(line)\n        elif line_stripped in seen and line_stripped.startswith(\"DO\"):\n            break  # stop at repeated section header\n\n    steps = \"\\n\".join(clean_lines).strip()\n\n    return {\n        \"situation\" : situation,\n        \"first_aid\" : steps,\n        \"boundary\"  : \"Bare-hands only. No medication. No clinical procedures.\",\n    }\n\n\n# TOOL 2: check_urgency\n# Rule-based keyword scoring. Returns spoken-language phrase only.\n# Never uses clinical labels like URGENT/CRITICAL with helpers.\n\n\nURGENCY_KEYWORDS = {\n    \"immediate\": [\n        \"chest pain\", \"difficulty breathing\", \"not breathing\",\n        \"unconscious\", \"severe bleeding\", \"seizure\", \"fits\",\n        \"convulsion\", \"snake bite\", \"poisoning\", \"stroke\",\n        \"paralysis\", \"face drooping\", \"newborn\", \"premature\",\n        \"labour\", \"childbirth\", \"not responding\", \"stopped breathing\",\n    ],\n    \"soon\": [\n        \"high fever\", \"fever three days\", \"fever 3 days\",\n        \"burn\", \"blistering\", \"vomiting\", \"dehydration\",\n        \"not eating\", \"weakness\", \"fracture\", \"broken\",\n        \"swelling\", \"wound\", \"injury\", \"dizziness\",\n        \"rash\", \"not drinking\",\n    ],\n    \"routine\": [\n        \"mild fever\", \"cold\", \"cough\", \"minor rash\",\n        \"minor pain\", \"headache\", \"fatigue\", \"tiredness\",\n        \"runny nose\",\n    ],\n}\n\nURGENCY_PHRASES = {\n    \"immediate\": (\n        \"This person needs to reach a doctor or hospital RIGHT NOW. \"\n        \"Please call 108 immediately â€” do not wait.\"\n    ),\n    \"soon\": (\n        \"This person should be seen by a health worker today. \"\n        \"Please take them to the nearest health centre as soon as possible.\"\n    ),\n    \"routine\": (\n        \"This does not seem like an emergency, but the person should still \"\n        \"visit a health worker within the next day or two.\"\n    ),\n}\n\n\ndef check_urgency(symptoms: str) -> dict:\n    \"\"\"\n    Tool 2: Keyword-based urgency scoring.\n    Returns spoken-language phrase â€” never clinical labels.\n    \"\"\"\n    text  = symptoms.lower()\n    score = {\"immediate\": 0, \"soon\": 0, \"routine\": 0}\n\n    for level, keywords in URGENCY_KEYWORDS.items():\n        for kw in keywords:\n            if kw in text:\n                score[level] += 1\n\n    if score[\"immediate\"] > 0:\n        level = \"immediate\"\n    elif score[\"soon\"] > 0:\n        level = \"soon\"\n    else:\n        level = \"routine\"\n\n    return {\n        \"urgency_level\"      : level,\n        \"urgency_phrase\"     : URGENCY_PHRASES[level],\n        \"triggered_keywords\" : [\n            kw for kw in URGENCY_KEYWORDS[level] if kw in text\n        ],\n    }\n\n\n\n# TOOL 3: lookup_nearest_facility\n# Hardcoded India PHC/CHC database by state.\n# Production: connect to NHA facility registry API or\n# offline GPS-based database.\n\nPHC_DATABASE = {\n    \"maharashtra\"    : [\"Pune District Hospital\", \"Nashik Civil Hospital\", \"PHC Sangamner\"],\n    \"rajasthan\"      : [\"SMS Hospital Jaipur\", \"CHC Barmer\", \"PHC Sirohi\"],\n    \"bihar\"          : [\"PMCH Patna\", \"Sadar Hospital Gaya\", \"PHC Nawada\"],\n    \"uttar pradesh\"  : [\"KGMU Lucknow\", \"District Hospital Varanasi\", \"CHC Ballia\"],\n    \"madhya pradesh\" : [\"Hamidia Hospital Bhopal\", \"MY Hospital Indore\", \"PHC Sehore\"],\n    \"west bengal\"    : [\"SSKM Kolkata\", \"District Hospital Malda\", \"PHC Murshidabad\"],\n    \"tamil nadu\"     : [\"GGH Chennai\", \"Coimbatore Medical College\", \"PHC Madurai\"],\n    \"gujarat\"        : [\"Civil Hospital Ahmedabad\", \"New Civil Hospital Surat\", \"PHC Anand\"],\n    \"karnataka\"      : [\"Victoria Hospital Bangalore\", \"District Hospital Hubli\", \"PHC Gulbarga\"],\n    \"andhra pradesh\" : [\"GGH Vijayawada\", \"RIMS Kadapa\", \"PHC Nellore\"],\n    \"telangana\"      : [\"Osmania GH Hyderabad\", \"District Hospital Warangal\", \"PHC Nizamabad\"],\n    \"kerala\"         : [\"GMC Thiruvananthapuram\", \"District Hospital Kozhikode\", \"PHC Thrissur\"],\n    \"punjab\"         : [\"PGIMER Chandigarh\", \"Civil Hospital Ludhiana\", \"PHC Amritsar\"],\n    \"haryana\"        : [\"PGIMS Rohtak\", \"Civil Hospital Faridabad\", \"PHC Hisar\"],\n    \"odisha\"         : [\"SCB Medical College Cuttack\", \"District Hospital Bhubaneswar\", \"PHC Puri\"],\n    \"jharkhand\"      : [\"RIMS Ranchi\", \"Sadar Hospital Dhanbad\", \"PHC Hazaribagh\"],\n    \"chhattisgarh\"   : [\"AIIMS Raipur\", \"District Hospital Bilaspur\", \"PHC Durg\"],\n    \"assam\"          : [\"GMCH Guwahati\", \"District Hospital Dibrugarh\", \"PHC Jorhat\"],\n    \"default\"        : [\n        \"Nearest Primary Health Centre (PHC)\",\n        \"Community Health Centre (CHC)\",\n        \"Health and Wellness Centre (HWC)\",\n        \"Call 108 â€” free emergency ambulance, available 24/7\",\n    ],\n}\n\n\ndef lookup_nearest_facility(region: str) -> dict:\n    \"\"\"\n    Tool 3: Returns nearest healthcare facilities for a given Indian region.\n    Includes Ayushman Bharat eligibility reminder.\n    \"\"\"\n    region_lower = region.lower().strip()\n    facilities   = None\n\n    for key, value in PHC_DATABASE.items():\n        if key in region_lower or region_lower in key:\n            facilities = value\n            break\n\n    if not facilities:\n        facilities = PHC_DATABASE[\"default\"]\n\n    return {\n        \"region\"          : region,\n        \"facilities\"      : facilities,\n        \"emergency\"       : \"108 â€” National Ambulance Service (free, 24/7)\",\n        \"scheme_reminder\" : (\n            \"If the family is eligible for Ayushman Bharat PM-JAY, \"\n            \"treatment at empanelled hospitals is free up to â‚¹5 lakhs per year.\"\n        ),\n    }\n\n\n\n# TOOL 4: generate_referral_note\n# Structured handoff note for the receiving doctor/health worker.\n# Ensures professional context from CHW is preserved.\n\n\ndef generate_referral_note(case_summary: str) -> dict:\n    \"\"\"\n    Tool 4: Generates a structured doctor handoff note.\n    Includes timestamp, CHW observations, and disclaimer.\n    \"\"\"\n    timestamp = datetime.now().strftime(\"%d %B %Y, %H:%M\")\n\n    note = (\n        f\"COMMUNITY HEALTH WORKER â€” REFERRAL NOTE\\n\"\n        f\"{'â”€' * 48}\\n\"\n        f\"Date & Time     : {timestamp}\\n\"\n        f\"Referred by     : Community Health Worker (CHW)\\n\"\n        f\"Tool            : Community Helper Guidebook (AI-assisted)\\n\"\n        f\"{'â”€' * 48}\\n\"\n        f\"CHW OBSERVATIONS:\\n{case_summary}\\n\"\n        f\"{'â”€' * 48}\\n\"\n        f\"NOTE TO RECEIVING DOCTOR:\\n\"\n        f\"The above was captured by a community health worker using\\n\"\n        f\"an AI-assisted guidebook. This is NOT a clinical assessment.\\n\"\n        f\"Please conduct a full clinical evaluation.\\n\"\n        f\"{'â”€' * 48}\\n\"\n    )\n\n    return {\n        \"referral_note\" : note,\n        \"timestamp\"     : timestamp,\n    }\n\n\n# Tool registry \nTOOLS = {\n    \"get_first_aid_steps\"     : get_first_aid_steps,\n    \"check_urgency\"           : check_urgency,\n    \"lookup_nearest_facility\" : lookup_nearest_facility,\n    \"generate_referral_note\"  : generate_referral_note,\n}\n\nprint(\"All 4 agentic tools ready:\")\nprint(\"   1. get_first_aid_steps(situation)\")\nprint(\"   2. check_urgency(symptoms)\")\nprint(\"   3. lookup_nearest_facility(region)\")\nprint(\"   4. generate_referral_note(case_summary)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T15:48:53.432590Z","iopub.execute_input":"2026-02-24T15:48:53.433485Z","iopub.status.idle":"2026-02-24T15:48:53.458386Z","shell.execute_reply.started":"2026-02-24T15:48:53.433453Z","shell.execute_reply":"2026-02-24T15:48:53.457471Z"}},"outputs":[{"name":"stdout","text":"All 4 agentic tools ready:\n   1. get_first_aid_steps(situation)\n   2. check_urgency(symptoms)\n   3. lookup_nearest_facility(region)\n   4. generate_referral_note(case_summary)\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# Module F â€” MedGemma Structured Agentic Pipeline\n\nAGENT_SYSTEM_PROMPT = \"\"\"You are a COMMUNITY HEALTH WORKER GUIDEBOOK ASSISTANT for low-resource Indian healthcare.\n\nYOUR ROLE:\n- Assist community health workers (CHWs), ASHA workers, NGO volunteers, and first-contact helpers.\n- You do NOT speak to patients directly.\n- You do NOT diagnose, prescribe medications, or recommend clinical treatments.\n- You help helpers know what to say, what first aid to give, and when to escalate.\n- Your guidance is grounded in Indian MoHFW and NHM regulations.\n\nYOU WILL BE GIVEN:\n- The helper's case report (translated to English)\n- Indian healthcare regulatory context (from MoHFW/NHM documents)\n- Pre-executed tool results: first aid steps, urgency level, nearest facility, referral note\n- Optional: medical image context\n\nYOUR ONLY JOB:\nUse ALL the provided information to write a complete Helper Guidance Pack.\nDo NOT call any tools. Do NOT add information not provided to you.\nDo NOT diagnose. Do NOT suggest medications.\n\nOUTPUT FORMAT â€” use this exactly:\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nHELPER GUIDANCE PACK\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nWHAT TO SAY TO THE PATIENT OR FAMILY:\n[2-3 calm, simple, spoken-friendly sentences. No jargon. Helper reads these aloud.]\n\nFIRST AID â€” DO THIS RIGHT NOW:\n[Use the first aid steps provided. Keep them simple and spoken-friendly.]\n\nWHAT NOT TO DO:\n[Use the do-not steps provided. Include harmful home remedies to avoid.]\n\nWHAT TO DO NEXT:\n[Use the facility name and emergency number provided. Be specific.]\n\nURGENCY:\n[Use the urgency phrase provided word for word. Do not change it.]\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSTRICT RULES:\n1. NEVER diagnose any condition.\n2. NEVER suggest medications, tablets, or injections.\n3. NEVER provide clinical treatment steps.\n4. NEVER change the urgency phrase â€” use it exactly as given.\n5. ALWAYS name the specific facility provided in WHAT TO DO NEXT.\n6. Use SIMPLE, SPOKEN language throughout.\n7. Every section must be present and complete.\"\"\"\n\n\ndef call_medgemma(\n    messages       : list[dict],\n    max_new_tokens : int   = 2048,\n    temperature    : float = 0.3,\n) -> str:\n    \"\"\"Helper to call MedGemma and decode only new tokens.\"\"\"\n    \n    # Apply chat template and get input ids as tensor\n    input_ids = tokenizer.apply_chat_template(\n        messages,\n        tokenize              = True,\n        add_generation_prompt = True,\n        return_tensors        = \"pt\",\n    )\n    \n    # Ensure we have a plain tensor not a BatchEncoding\n    if hasattr(input_ids, \"input_ids\"):\n        input_ids = input_ids.input_ids\n    \n    input_ids = input_ids.to(DEVICE)\n\n    # Truncate if too long for GPU memory (keep last 3000 tokens)\n    if input_ids.shape[-1] > 3000:\n        input_ids = input_ids[:, -3000:]\n        print(f\"  Prompt truncated to 3000 tokens to fit GPU memory.\")\n\n    with torch.no_grad():\n        output_ids = medgemma.generate(\n            input_ids,\n            max_new_tokens = max_new_tokens,\n            temperature    = temperature,\n            do_sample      = (temperature > 0),\n            pad_token_id   = tokenizer.eos_token_id,\n        )\n\n    new_tokens = output_ids[0][input_ids.shape[-1]:]\n    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n\n\ndef run_react_agent(\n    transcript    : str,\n    rag_context   : str,\n    image_context : dict | None = None,\n    helper_notes  : str         = \"\",\n    region        : str         = \"India\",\n    max_steps     : int         = 4,   # kept for compatibility, not used in pre-execution\n) -> tuple[str, list[dict]]:\n    \"\"\"\n    Structured agentic pipeline:\n    1. Pre-executes all 4 tools with case data (guaranteed, no loops)\n    2. Passes all tool results + RAG context to MedGemma in one synthesis call\n    3. MedGemma writes the complete Helper Guidance Pack\n\n    Why pre-execution over free tool selection:\n    - Eliminates infinite loops (tool called repeatedly with no progress)\n    - Guarantees all sections are populated with real tool data\n    - Prevents hallucinated urgency or first aid values\n    - Faster â€” 1 synthesis call instead of 6-10 loop iterations\n    - More reliable for a safety-critical healthcare application\n    \"\"\"\n    reasoning_trace = []\n\n    # â”€â”€ STEP 1: Pre-execute all 4 tools â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    print(\"   â†³ Pre-executing all 4 tools...\")\n    tool_results = {}\n\n    # Tool 1: First aid steps\n    try:\n        tool_results[\"first_aid\"] = get_first_aid_steps(transcript)\n        print(\"   âœ“ get_first_aid_steps\")\n    except Exception as e:\n        print(f\" get_first_aid_steps failed: {e}\")\n        tool_results[\"first_aid\"] = {\n            \"first_aid\": (\n                \"DO THIS NOW:\\n\"\n                \"1. Keep the person calm and still.\\n\"\n                \"2. Move them to a safe, shaded area.\\n\"\n                \"3. Loosen any tight clothing.\\n\"\n                \"4. Stay with them until help arrives.\\n\\n\"\n                \"DO NOT DO:\\n\"\n                \"1. Do not give any medication.\\n\"\n                \"2. Do not leave them alone.\"\n            )\n        }\n\n    reasoning_trace.append({\n        \"step\"       : 1,\n        \"agent_output\": \"Tool called: get_first_aid_steps\",\n        \"tool_called\": \"get_first_aid_steps\",\n        \"tool_result\": tool_results[\"first_aid\"],\n    })\n\n    # Tool 2: Urgency check\n    try:\n        tool_results[\"urgency\"] = check_urgency(transcript + \" \" + helper_notes)\n        print(\"   âœ“ check_urgency\")\n    except Exception as e:\n        print(f\" check_urgency failed: {e}\")\n        tool_results[\"urgency\"] = {\n            \"urgency_phrase\": (\n                \"This person should be seen by a health worker as soon as possible.\"\n            )\n        }\n\n    reasoning_trace.append({\n        \"step\"       : 2,\n        \"agent_output\": \"Tool called: check_urgency\",\n        \"tool_called\": \"check_urgency\",\n        \"tool_result\": tool_results[\"urgency\"],\n    })\n\n    # Tool 3: Nearest facility\n    try:\n        tool_results[\"facility\"] = lookup_nearest_facility(region)\n        print(\"   âœ“ lookup_nearest_facility\")\n    except Exception as e:\n        print(f\" lookup_nearest_facility failed: {e}\")\n        tool_results[\"facility\"] = {\n            \"facilities\": [\"Nearest Primary Health Centre (PHC)\", \"Community Health Centre (CHC)\"],\n            \"emergency\" : \"108 â€” free ambulance, 24/7\",\n            \"scheme_reminder\": \"Check Ayushman Bharat PM-JAY eligibility.\",\n        }\n\n    reasoning_trace.append({\n        \"step\"       : 3,\n        \"agent_output\": \"Tool called: lookup_nearest_facility\",\n        \"tool_called\": \"lookup_nearest_facility\",\n        \"tool_result\": tool_results[\"facility\"],\n    })\n\n    # Tool 4: Referral note\n    try:\n        tool_results[\"referral\"] = generate_referral_note(\n            f\"{transcript}\\nHelper notes: {helper_notes}\\nRegion: {region}\"\n        )\n        print(\"   âœ“ generate_referral_note\")\n    except Exception as e:\n        print(f\" generate_referral_note failed: {e}\")\n        tool_results[\"referral\"] = {\n            \"referral_note\": f\"CHW observation: {transcript}\\nHelper notes: {helper_notes}\"\n        }\n\n    reasoning_trace.append({\n        \"step\"       : 4,\n        \"agent_output\": \"Tool called: generate_referral_note\",\n        \"tool_called\": \"generate_referral_note\",\n        \"tool_result\": tool_results[\"referral\"],\n    })\n\n    # â”€â”€ STEP 2: Build image context â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    img_text = \"\"\n    if image_context:\n        img_text = f\"\\n\\nIMAGE CONTEXT (SigLIP):\\n{image_context['context_for_llm']}\"\n\n    notes_text = \"\"\n    if helper_notes.strip():\n        notes_text = f\"\\n\\nHELPER ADDITIONAL OBSERVATIONS:\\n{helper_notes.strip()}\"\n\n    # â”€â”€ STEP 3: Build synthesis prompt with all tool results â”€â”€â”€â”€â”€â”€\n    synthesis_message = (\n      \n       f\"CASE: \\\"{transcript}\\\"\\n\"\n       f\"Region: {region}\\n\"\n       f\"Helper notes: {helper_notes}\\n\\n\"\n       f\"REGULATORY CONTEXT:\\n{rag_context[:800]}\\n\\n\"\n       f\"FIRST AID STEPS:\\n{tool_results['first_aid']['first_aid'][:400]}\\n\\n\"\n       f\"URGENCY: {tool_results['urgency']['urgency_phrase']}\\n\\n\"\n       f\"NEAREST FACILITY: {', '.join(tool_results['facility']['facilities'][:2])}\\n\"\n       f\"EMERGENCY: {tool_results['facility']['emergency']}\\n\\n\"\n       f\"Write the complete Helper Guidance Pack with all 5 sections now.\"\n       \n    )\n\n    messages = [\n        {\"role\": \"system\", \"content\": AGENT_SYSTEM_PROMPT},\n        {\"role\": \"user\",   \"content\": synthesis_message},\n    ]\n\n    # â”€â”€ STEP 4: Single MedGemma synthesis call â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    print(\"   â†³ MedGemma synthesising guidance pack...\")\n    final_answer = call_medgemma(messages, max_new_tokens=2048, temperature=0.3)\n\n    # Strip Final Answer: prefix if present\n    if \"Final Answer:\" in final_answer:\n        final_answer = final_answer.split(\"Final Answer:\")[-1].strip()\n\n    # â”€â”€ STEP 5: If output looks truncated, retry once â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if len(final_answer.split()) < 80:\n        print(\" Output looks short â€” retrying with explicit section request...\")\n        messages.append({\"role\": \"assistant\", \"content\": final_answer})\n        messages.append({\n            \"role\"    : \"user\",\n            \"content\" : (\n                \"Your response was incomplete. Write the FULL Helper Guidance Pack \"\n                \"with ALL 5 sections:\\n\"\n                \"1. WHAT TO SAY TO THE PATIENT OR FAMILY\\n\"\n                \"2. FIRST AID â€” DO THIS RIGHT NOW\\n\"\n                \"3. WHAT NOT TO DO\\n\"\n                \"4. WHAT TO DO NEXT\\n\"\n                \"5. URGENCY\\n\"\n                \"Use the tool results provided. All sections must be complete.\"\n            ),\n        })\n        final_answer = call_medgemma(messages, max_new_tokens=2048, temperature=0.3)\n\n    reasoning_trace.append({\n        \"step\"        : 5,\n        \"agent_output\": final_answer,\n    })\n\n    print(f\"   âœ“ Guidance pack generated ({len(final_answer.split())} words).\")\n    return final_answer, reasoning_trace\n\n\nprint(\" MedGemma structured agentic pipeline ready.\")\nprint(\"   Tools: get_first_aid_steps â†’ check_urgency â†’ lookup_nearest_facility â†’ generate_referral_note â†’ synthesis\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T15:51:07.118757Z","iopub.execute_input":"2026-02-24T15:51:07.119507Z","iopub.status.idle":"2026-02-24T15:51:07.138033Z","shell.execute_reply.started":"2026-02-24T15:51:07.119476Z","shell.execute_reply":"2026-02-24T15:51:07.137246Z"}},"outputs":[{"name":"stdout","text":" MedGemma structured agentic pipeline ready.\n   Tools: get_first_aid_steps â†’ check_urgency â†’ lookup_nearest_facility â†’ generate_referral_note â†’ synthesis\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# Module G â€” Safety Guardrails\n\nFORBIDDEN_PATTERNS = [\n    \"you have\",\n    \"the patient has\",\n    \"this is a case of\",\n    \"this looks like\",\n    \"take this medicine\",\n    \"take paracetamol\",\n    \"take ibuprofen\",\n    \"give them medicine\",\n    \"apply this cream\",\n    \"administer\",\n    \"prescribe\",\n    \"diagnos\",\n    \"it will get better\",\n    \"nothing to worry\",\n    \"don't worry it's fine\",\n]\n\nIMMEDIATE_ESCALATION_KEYWORDS = [\n    \"chest pain\", \"difficulty breathing\", \"not breathing\",\n    \"unconscious\", \"severe bleeding\", \"seizure\", \"fits\",\n    \"convulsion\", \"snake bite\", \"poisoning\", \"stroke\",\n    \"paralysis\", \"newborn not breathing\", \"premature birth\",\n    \"not responding\",\n]\n\n\ndef apply_guardrails(\n    guidance   : str,\n    transcript : str,\n) -> tuple[str, list[str], bool]:\n    \"\"\"\n    Post-generation safety filter.\n    Scans for forbidden phrases and immediate escalation triggers.\n    Returns (guidance, warnings, requires_immediate_escalation).\n    \"\"\"\n    warnings_list      = []\n    requires_immediate = False\n\n    for pattern in FORBIDDEN_PATTERNS:\n        if pattern.lower() in guidance.lower():\n            warnings_list.append(\n                f\" Review needed â€” phrase detected: '{pattern}'\"\n            )\n\n    for kw in IMMEDIATE_ESCALATION_KEYWORDS:\n        if kw in transcript.lower():\n            requires_immediate = True\n            break\n\n    if requires_immediate:\n        guidance += (\n            \"\\n\\nIMMEDIATE ACTION REQUIRED\\n\"\n            \"Signs of a serious emergency have been detected.\\n\"\n            \"Call 108 immediately â€” this is a free national ambulance service.\\n\"\n            \"Do not wait. Take the person to the nearest hospital now.\"\n        )\n\n    return guidance, warnings_list, requires_immediate\n\n\nprint(\"Safety guardrails ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T14:42:04.285455Z","iopub.execute_input":"2026-02-24T14:42:04.285758Z","iopub.status.idle":"2026-02-24T14:42:04.292958Z","shell.execute_reply.started":"2026-02-24T14:42:04.285730Z","shell.execute_reply":"2026-02-24T14:42:04.292152Z"}},"outputs":[{"name":"stdout","text":"Safety guardrails ready.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Module H â€” Full Pipeline Orchestrator\n\ndef run_full_pipeline(\n    audio_path      : str | None         = None,\n    text_input      : str                = \"\",\n    confirmed_text  : str                = \"\",\n    image           : Image.Image | None = None,\n    helper_notes    : str                = \"\",\n    input_language  : str                = \"Hindi\",\n    output_language : str                = \"Hindi\",\n    region          : str                = \"India\",\n    verbose         : bool               = True,\n) -> dict:\n    \"\"\"\n    End-to-end pipeline:\n    Audio/Text â†’ STT â†’ Confirmation â†’ Translation â†’\n    RAG Retrieval â†’ Image Encoding â†’ MedGemma ReAct Agent â†’\n    Guardrails â†’ Translation â†’ Helper Guidance Pack\n    \"\"\"\n    result     = {}\n    input_mode = \"audio\" if audio_path else \"text\"\n    result[\"input_mode\"] = input_mode\n\n    # â”€â”€ STEP 1: Speech-to-Text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if input_mode == \"audio\":\n        if verbose: print(\"[1/8] Transcribing audio with Whisper...\")\n        lang_code                   = SUPPORTED_LANGUAGES.get(input_language, \"hi\")\n        transcript, detected_lang   = transcribe_audio(audio_path, lang_code)\n        result[\"transcript_raw\"]    = transcript\n        result[\"detected_language\"] = detected_lang\n        if verbose: print(f\"   âœ“ Transcript: {transcript[:80]}...\")\n    else:\n        transcript                  = text_input.strip()\n        result[\"transcript_raw\"]    = transcript\n        result[\"detected_language\"] = SUPPORTED_LANGUAGES.get(input_language, \"hi\")\n        if verbose: print(\" [1/8] Text input â€” STT skipped.\")\n\n    # â”€â”€ STEP 2: Confirmation Layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if verbose: print(\"[2/8] Confirmation layer...\")\n    confirmation        = prepare_confirmation(transcript, input_mode)\n    final_transcript    = confirmed_text.strip() if confirmed_text.strip() else transcript\n    result[\"transcript_confirmed\"] = final_transcript\n    result[\"confirmation\"]         = confirmation\n\n    # â”€â”€ STEP 3: Translate to English â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if verbose: print(\" [3/8] Translating to English (IndicTrans2)...\")\n    english_text, trans_src         = translate_to_english(final_transcript, input_language)\n    result[\"transcript_english\"]    = english_text\n    result[\"translation_source\"]    = trans_src\n    if verbose: print(f\"   âœ“ {english_text[:80]}...\")\n\n    # â”€â”€ STEP 4: RAG Retrieval â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if verbose: print(\"[4/8] Retrieving Indian healthcare regulations (RAG)...\")\n    rag_query   = f\"{english_text} {helper_notes} region: {region}\"\n    rag_context = retrieve_context(rag_query, rag_index, rag_doc_store, top_k=4)\n    result[\"rag_context\"] = rag_context\n    if verbose: print(f\"   âœ“ Retrieved {len(rag_context.split())} words of regulatory context.\")\n\n    # â”€â”€ STEP 5: Image Encoding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    image_context = None\n    if image is not None:\n        if verbose: print(\" [5/8] Encoding medical image (SigLIP)...\")\n        image_context = encode_image(image)\n        result[\"image_context\"] = image_context\n        if verbose: print(f\"   âœ“ Detected: {image_context['image_type']} ({image_context['confidence']:.0%})\")\n    else:\n        if verbose: print(\" [5/8] No image â€” skipped.\")\n        result[\"image_context\"] = None\n\n    # â”€â”€ STEP 6: MedGemma ReAct Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if verbose: print(\"[6/8] Running MedGemma ReAct agent (RAG-grounded)...\")\n    guidance_en, reasoning_trace = run_react_agent(\n        transcript    = english_text,\n        rag_context   = rag_context,\n        image_context = image_context,\n        helper_notes  = helper_notes,\n        region        = region,\n    )\n    result[\"guidance_english\"]  = guidance_en\n    result[\"reasoning_trace\"]   = reasoning_trace\n    if verbose: print(f\"   âœ“ Agent completed in {len(reasoning_trace)} reasoning steps.\")\n\n    # â”€â”€ STEP 7: Safety Guardrails â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if verbose: print(\" [7/8] Applying safety guardrails...\")\n    safe_guidance, warnings, immediate = apply_guardrails(guidance_en, english_text)\n    result[\"safety_warnings\"]          = warnings\n    result[\"requires_immediate\"]       = immediate\n    if immediate and verbose:\n        print(\"   IMMEDIATE ESCALATION TRIGGERED.\")\n\n    # â”€â”€ STEP 8: Translate Output to Local Language â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    if verbose: print(f\" [8/8] Translating output to {output_language}...\")\n\n    final_pack_en = (\n        safe_guidance\n        + DISCLAIMER\n    )\n\n    final_pack_local, _ = translate_from_english(final_pack_en, output_language)\n\n    result[\"guidance_pack_english\"] = final_pack_en\n    result[\"guidance_pack_local\"]   = final_pack_local\n    result[\"output_language\"]       = output_language\n\n    if verbose:\n       print(\"\\n\" + final_pack_local)\n       for w in warnings:\n           print(w)\n\n    return result\n\n\nprint(\"Full pipeline orchestrator ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T16:03:11.950793Z","iopub.execute_input":"2026-02-24T16:03:11.951388Z","iopub.status.idle":"2026-02-24T16:03:11.967238Z","shell.execute_reply.started":"2026-02-24T16:03:11.951359Z","shell.execute_reply":"2026-02-24T16:03:11.966488Z"}},"outputs":[{"name":"stdout","text":"Full pipeline orchestrator ready.\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# Synthetic Demo Scenarios\n\nDEMO_SCENARIOS = [\n    {\n        \"id\"             : 1,\n        \"title\"          : \"Scenario 1 â€” Child with Prolonged Fever (Hindi output)\",\n        \"input_language\" : \"Hindi\",\n        \"output_language\": \"Hindi\",\n        \"text_input\"     : (\n            \"Baccha teen din se bukhaar mein hai. Woh kuch kha nahi raha. \"\n            \"Raat ko kaafi zyada bukhaar badh jaata hai. Woh paanch saal ka hai.\"\n        ),\n        \"helper_notes\"   : \"Child looks very weak. Family is 40km from nearest hospital. No vehicle available.\",\n        \"region\"         : \"Rajasthan\",\n        \"image\"          : None,\n    },\n    {\n        \"id\"             : 2,\n        \"title\"          : \"Scenario 2 â€” Burn Injury with Image (Hindi output)\",\n        \"input_language\" : \"English\",\n        \"output_language\": \"Hindi\",\n        \"text_input\"     : (\n            \"The woman's hand got burned while cooking. There is redness and \"\n            \"blistering on the palm. She is in a lot of pain. \"\n            \"It happened two hours ago. Someone applied mustard oil â€” we told them to stop.\"\n        ),\n        \"helper_notes\"   : \"No doctor in the village. Nearest PHC is 25km away. Family has Ayushman Bharat card.\",\n        \"region\"         : \"Maharashtra\",\n        \"image\"          : \"placeholder_burn\",\n    },\n    {\n        \"id\"             : 3,\n        \"title\"          : \"Scenario 3 â€” Chest Pain & Breathlessness (Immediate escalation, English output)\",\n        \"input_language\" : \"English\",\n        \"output_language\": \"English\",\n        \"text_input\"     : (\n            \"An elderly man around 65 years old is having chest pain and \"\n            \"difficulty breathing. He is sweating heavily and his left arm is hurting. \"\n            \"His lips look pale. The pain started about 30 minutes ago.\"\n        ),\n        \"helper_notes\"   : \"108 ambulance called but 45 minutes away. No oxygen available. Son is present.\",\n        \"region\"         : \"Uttar Pradesh\",\n        \"image\"          : None,\n    },\n]\n\n\ndef run_demo(scenario: dict) -> dict:\n    print(f\"\\n{'=' * 70}\")\n    print(f\"{scenario['title']}\")\n    print(f\"{'=' * 70}\")\n\n    image = None\n    if scenario[\"image\"] == \"placeholder_burn\":\n        image = Image.new(\"RGB\", (400, 300), color=(255, 200, 180))\n        draw  = ImageDraw.Draw(image)\n        draw.text(\n            (200, 150),\n            \"Burn injury image (demo placeholder)\",\n            fill   = (180, 60, 30),\n            anchor = \"mm\",\n        )\n\n    return run_full_pipeline(\n        text_input      = scenario[\"text_input\"],\n        image           = image,\n        helper_notes    = scenario[\"helper_notes\"],\n        input_language  = scenario[\"input_language\"],\n        output_language = scenario[\"output_language\"],\n        region          = scenario[\"region\"],\n        verbose         = True,\n    )\n\n\nprint(\" Running demo scenarios...\\n\")\nall_results = []\nfor scenario in DEMO_SCENARIOS:\n    res = run_demo(scenario)\n    all_results.append(res)\n    print()\n\nprint(\"All demo scenarios completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T16:03:16.498327Z","iopub.execute_input":"2026-02-24T16:03:16.498905Z","iopub.status.idle":"2026-02-24T16:06:23.454785Z","shell.execute_reply.started":"2026-02-24T16:03:16.498880Z","shell.execute_reply":"2026-02-24T16:06:23.454050Z"}},"outputs":[{"name":"stdout","text":" Running demo scenarios...\n\n\n======================================================================\nScenario 1 â€” Child with Prolonged Fever (Hindi output)\n======================================================================\n [1/8] Text input â€” STT skipped.\n[2/8] Confirmation layer...\n [3/8] Translating to English (IndicTrans2)...\n   âœ“ The child has been suffering from fever since day one. He was not eating. There ...\n[4/8] Retrieving Indian healthcare regulations (RAG)...\n   âœ“ Retrieved 1565 words of regulatory context.\n [5/8] No image â€” skipped.\n[6/8] Running MedGemma ReAct agent (RAG-grounded)...\n   â†³ Pre-executing all 4 tools...\n   âœ“ get_first_aid_steps\n   âœ“ check_urgency\n   âœ“ lookup_nearest_facility\n   âœ“ generate_referral_note\n   â†³ MedGemma synthesising guidance pack...\n   âœ“ Guidance pack generated (126 words).\n   âœ“ Agent completed in 5 reasoning steps.\n [7/8] Applying safety guardrails...\n [8/8] Translating output to Hindi...\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nà¤¸à¤¹à¤¾à¤¯à¤• à¤®à¤¾à¤°à¥à¤—à¤¦à¤°à¥à¤¶à¤¨ à¤ªà¥ˆà¤•\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nà¤®à¤°à¥€à¤œà¤¼ à¤¯à¤¾ à¤ªà¤°à¤¿à¤µà¤¾à¤° à¤¸à¥‡ à¤•à¥à¤¯à¤¾ à¤•à¤¹à¥‡à¤‚:\nà¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤®à¥ˆà¤‚ à¤®à¤¦à¤¦ à¤•à¥‡ à¤²à¤¿à¤ à¤¯à¤¹à¤¾à¤‚ à¤¹à¥‚à¤‚à¥¤ à¤®à¥ˆà¤‚ à¤¸à¤®à¤à¤¤à¤¾ à¤¹à¥‚à¤‚ à¤•à¤¿ à¤¬à¤šà¥à¤šà¤¾ à¤¬à¥à¤–à¤¾à¤° à¤¸à¥‡ à¤…à¤¸à¥à¤µà¤¸à¥à¤¥ à¤¹à¥ˆà¥¤ à¤¹à¤®à¥‡à¤‚ à¤‰à¤¸à¥‡ à¤¯à¤¥à¤¾à¤¶à¥€à¤˜à¥à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤¸à¥à¤µà¤¿à¤§à¤¾ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤¤à¤• à¤ªà¤¹à¥à¤‚à¤šà¤¾à¤¨à¥‡ à¤•à¥€ à¤œà¤°à¥‚à¤°à¤¤ à¤¹à¥ˆà¥¤\n\nà¤ªà¥à¤°à¤¾à¤¥à¤®à¤¿à¤• à¤‰à¤ªà¤šà¤¾à¤° - à¤¯à¤¹ à¤…à¤­à¥€ à¤•à¤°à¥‡à¤‚:\n1. à¤¬à¤šà¥à¤šà¥‡ à¤•à¤¾ à¤¤à¤¾à¤ªà¤®à¤¾à¤¨ à¤œà¤¾à¤‚à¤šà¥‡à¤‚à¥¤\n2. à¤¬à¤šà¥à¤šà¥‡ à¤•à¥‹ à¤›à¥‹à¤Ÿà¥‡-à¤›à¥‹à¤Ÿà¥‡ à¤˜à¥‚à¤‚à¤Ÿ à¤®à¥‡à¤‚ à¤ªà¤¾à¤¨à¥€ à¤ªà¤¿à¤²à¤¾à¤à¤‚à¥¤\n3. à¤¬à¤šà¥à¤šà¥‡ à¤•à¥‹ à¤ à¤‚à¤¡à¤¾ à¤°à¤–à¥‡à¤‚.\n\nà¤•à¥à¤¯à¤¾ à¤¨ à¤•à¤°à¥‡à¤‚:\n1. à¤•à¥‹à¤ˆ à¤¦à¤µà¤¾ à¤¦à¥€à¤œà¤¿à¤.\n2. à¤•à¥‹à¤ˆ à¤­à¥€ à¤¨à¥ˆà¤¦à¤¾à¤¨à¤¿à¤• â€‹â€‹à¤ªà¥à¤°à¤•à¥à¤°à¤¿à¤¯à¤¾ à¤¨à¤¿à¤·à¥à¤ªà¤¾à¤¦à¤¿à¤¤ à¤•à¤°à¥‡à¤‚à¥¤\n\nà¤†à¤—à¥‡ à¤•à¥à¤¯à¤¾ à¤•à¤°à¥‡à¤‚:\nà¤œà¤¿à¤¤à¤¨à¥€ à¤œà¤²à¥à¤¦à¥€ à¤¹à¥‹ à¤¸à¤•à¥‡ à¤¬à¤šà¥à¤šà¥‡ à¤•à¥‹ à¤à¤¸à¤à¤®à¤à¤¸ à¤¹à¥‰à¤¸à¥à¤ªà¤¿à¤Ÿà¤² à¤œà¤¯à¤ªà¥à¤° à¤¯à¤¾ à¤¸à¥€.à¤à¤š.à¤¸à¥€.à¤¬à¤¾à¤¡à¤¼à¤®à¥‡à¤° à¤²à¥‡ à¤œà¤¾à¤à¤‚à¥¤ à¤à¤®à¥à¤¬à¥à¤²à¥‡à¤‚à¤¸ à¤•à¥‡ à¤²à¤¿à¤ 108 à¤ªà¤° à¤•à¥‰à¤² à¤•à¤°à¥‡à¤‚à¥¤\n\nà¤…à¤¤à¥à¤¯à¤¾à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾: à¤‡à¤¸ à¤µà¥à¤¯à¤•à¥à¤¤à¤¿ à¤•à¥‹ à¤†à¤œ à¤¹à¥€ à¤•à¤¿à¤¸à¥€ à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤•à¤¾à¤°à¥à¤¯à¤•à¤°à¥à¤¤à¤¾ à¤•à¥‹ à¤¦à¥‡à¤–à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤à¥¤ à¤•à¥ƒà¤ªà¤¯à¤¾ à¤‰à¤¨à¥à¤¹à¥‡à¤‚ à¤¯à¤¥à¤¾à¤¶à¥€à¤˜à¥à¤° à¤¨à¤œà¤¦à¥€à¤•à¥€ à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤²à¥‡ à¤œà¤¾à¤à¤‚à¥¤\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤¸à¥‚à¤šà¤¨à¤¾\nà¤¯à¤¹ à¤•à¥‡à¤µà¤² à¤¸à¤¾à¤®à¥à¤¦à¤¾à¤¯à¤¿à¤• à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤•à¤¾à¤°à¥à¤¯à¤•à¤°à¥à¤¤à¤¾à¤“à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¤¹à¤¾à¤¯à¤• à¤®à¤¾à¤°à¥à¤—à¤¦à¤°à¥à¤¶à¤¨ à¤¹à¥ˆà¥¤\nà¤¯à¤¹ à¤¨à¥ˆà¤¦à¤¾à¤¨à¤¿à¤• â€‹â€‹à¤¸à¤²à¤¾à¤¹ à¤¯à¤¾ à¤šà¤¿à¤•à¤¿à¤¤à¥à¤¸à¥€à¤¯ à¤ªà¤°à¥€à¤•à¥à¤·à¤£ à¤•à¤¾ à¤¸à¥à¤¥à¤¾à¤¨ à¤¨à¤¹à¥€à¤‚ à¤²à¥‡à¤¤à¤¾à¥¤\nà¤¹à¤®à¥‡à¤¶à¤¾ à¤•à¤¿à¤¸à¥€ à¤¯à¥‹à¤—à¥à¤¯ à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤¦à¥‡à¤–à¤­à¤¾à¤² à¤ªà¥‡à¤¶à¥‡à¤µà¤° à¤¸à¥‡ à¤¸à¤‚à¤ªà¤°à¥à¤• à¤•à¤°à¥‡à¤‚à¥¤\nà¤†à¤ªà¤¾à¤¤à¤•à¤¾à¤²à¥€à¤¨ à¤à¤®à¥à¤¬à¥à¤²à¥‡à¤‚à¤¸: 108 à¤ªà¤° à¤•à¥‰à¤² à¤•à¤°à¥‡à¤‚ (à¤¨à¤¿à¤ƒà¤¶à¥à¤²à¥à¤•, 24/7)\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\n======================================================================\nScenario 2 â€” Burn Injury with Image (Hindi output)\n======================================================================\n [1/8] Text input â€” STT skipped.\n[2/8] Confirmation layer...\n [3/8] Translating to English (IndicTrans2)...\n   âœ“ The woman's hand got burned while cooking. There is redness and blistering on th...\n[4/8] Retrieving Indian healthcare regulations (RAG)...\n   âœ“ Retrieved 1367 words of regulatory context.\n [5/8] Encoding medical image (SigLIP)...\n   âœ“ Detected: burn injury on skin (93%)\n[6/8] Running MedGemma ReAct agent (RAG-grounded)...\n   â†³ Pre-executing all 4 tools...\n   âœ“ get_first_aid_steps\n   âœ“ check_urgency\n   âœ“ lookup_nearest_facility\n   âœ“ generate_referral_note\n   â†³ MedGemma synthesising guidance pack...\n   âœ“ Guidance pack generated (190 words).\n   âœ“ Agent completed in 5 reasoning steps.\n [7/8] Applying safety guardrails...\n [8/8] Translating output to Hindi...\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nà¤¸à¤¹à¤¾à¤¯à¤• à¤®à¤¾à¤°à¥à¤—à¤¦à¤°à¥à¤¶à¤¨ à¤ªà¥ˆà¤•\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nà¤®à¤°à¥€à¤œà¤¼ à¤¯à¤¾ à¤ªà¤°à¤¿à¤µà¤¾à¤° à¤¸à¥‡ à¤•à¥à¤¯à¤¾ à¤•à¤¹à¥‡à¤‚:\n\"à¤¨à¤®à¤¸à¥à¤•à¤¾à¤°, à¤®à¥ˆà¤‚ à¤†à¤ªà¤•à¥€ à¤®à¤¦à¤¦ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¯à¤¹à¤¾à¤‚ à¤¹à¥‚à¤‚à¥¤ à¤†à¤ªà¤•à¤¾ à¤¹à¤¾à¤¥ à¤œà¤² à¤—à¤¯à¤¾ à¤¹à¥ˆ, à¤”à¤° à¤¹à¤®à¥‡à¤‚ à¤‡à¤¸à¤•à¥€ à¤¦à¥‡à¤–à¤­à¤¾à¤² à¤•à¤°à¤¨à¥€ à¤¹à¥‹à¤—à¥€à¥¤ à¤†à¤‡à¤ à¤‡à¤¸à¥‡ à¤¸à¤¾à¤« à¤•à¤°à¥‡à¤‚ à¤”à¤° à¤¦à¥‡à¤–à¥‡à¤‚ à¤•à¤¿ à¤•à¥à¤¯à¤¾ à¤¹à¤® à¤‡à¤¸à¥‡ à¤¬à¥‡à¤¹à¤¤à¤° à¤¬à¤¨à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤\"\n\nà¤ªà¥à¤°à¤¾à¤¥à¤®à¤¿à¤• à¤‰à¤ªà¤šà¤¾à¤° - à¤¯à¤¹ à¤…à¤­à¥€ à¤•à¤°à¥‡à¤‚:\n1. à¤œà¤²à¥‡ à¤¹à¥à¤ à¤¸à¥à¤¥à¤¾à¤¨ à¤•à¥‹ à¤¸à¤¾à¤« à¤ªà¤¾à¤¨à¥€ à¤¸à¥‡ à¤§à¥‹à¤à¤‚à¥¤\n2. à¤œà¤²à¥‡ à¤¹à¥à¤ à¤¸à¥à¤¥à¤¾à¤¨ à¤ªà¤° à¤¸à¤¾à¤«, à¤¸à¥‚à¤–à¤¾ à¤•à¤ªà¤¡à¤¼à¤¾ à¤²à¤—à¤¾à¤à¤‚à¥¤\n3. à¤œà¤²à¥‡ à¤¹à¥à¤ à¤¹à¤¾à¤¥ à¤•à¥‹ à¤Šà¤ªà¤° à¤‰à¤ à¤¾à¤à¤‚à¥¤\n\nà¤•à¥à¤¯à¤¾ à¤¨ à¤•à¤°à¥‡à¤‚:\n1. à¤”à¤° à¤¸à¤°à¤¸à¥‹à¤‚ à¤•à¤¾ à¤¤à¥‡à¤² à¤²à¤—à¤¾ à¤²à¥‡à¤‚.\n2. à¤œà¤²à¥‡ à¤¹à¥à¤ à¤¸à¥à¤¥à¤¾à¤¨ à¤•à¥‹ à¤ªà¤Ÿà¥à¤Ÿà¥€ à¤¸à¥‡ à¤¢à¤• à¤¦à¥‡à¤‚à¥¤\n\nà¤†à¤—à¥‡ à¤•à¥à¤¯à¤¾ à¤•à¤°à¥‡à¤‚:\nà¤®à¤°à¥€à¤œ à¤•à¥‹ à¤ªà¥à¤£à¥‡ à¤œà¤¿à¤²à¤¾ à¤…à¤¸à¥à¤ªà¤¤à¤¾à¤² à¤²à¥‡ à¤œà¤¾à¤à¤‚à¥¤ à¤à¤®à¥à¤¬à¥à¤²à¥‡à¤‚à¤¸ à¤•à¥‡ à¤²à¤¿à¤ 108 à¤ªà¤° à¤•à¥‰à¤² à¤•à¤°à¥‡à¤‚à¥¤\n\nà¤…à¤¤à¥à¤¯à¤¾à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾: à¤‡à¤¸ à¤µà¥à¤¯à¤•à¥à¤¤à¤¿ à¤•à¥‹ à¤†à¤œ à¤¹à¥€ à¤•à¤¿à¤¸à¥€ à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤•à¤¾à¤°à¥à¤¯à¤•à¤°à¥à¤¤à¤¾ à¤•à¥‹ à¤¦à¥‡à¤–à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤à¥¤ à¤•à¥ƒà¤ªà¤¯à¤¾ à¤‰à¤¨à¥à¤¹à¥‡à¤‚ à¤¯à¤¥à¤¾à¤¶à¥€à¤˜à¥à¤° à¤¨à¤œà¤¦à¥€à¤•à¥€ à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤²à¥‡ à¤œà¤¾à¤à¤‚à¥¤\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nà¤¸à¤–à¥à¤¤ à¤¨à¤¿à¤¯à¤®:\n1. à¤•à¤­à¥€ à¤­à¥€ à¤•à¤¿à¤¸à¥€ à¤­à¥€ à¤¸à¥à¤¥à¤¿à¤¤à¤¿ à¤•à¤¾ à¤¨à¤¿à¤¦à¤¾à¤¨ à¤¨ à¤•à¤°à¥‡à¤‚à¥¤\n2. à¤•à¤­à¥€ à¤­à¥€ à¤¦à¤µà¤¾à¤à¤, à¤—à¥‹à¤²à¤¿à¤¯à¤¾à¤ à¤¯à¤¾ à¤‡à¤‚à¤œà¥‡à¤•à¥à¤¶à¤¨ à¤•à¤¾ à¤¸à¥à¤à¤¾à¤µ à¤¨ à¤¦à¥‡à¤‚à¥¤\n3. à¤•à¤­à¥€ à¤­à¥€ à¤¨à¥ˆà¤¦à¤¾à¤¨à¤¿à¤• â€‹â€‹à¤‰à¤ªà¤šà¤¾à¤° à¤šà¤°à¤£ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤¨ à¤•à¤°à¥‡à¤‚à¥¤\n4. à¤…à¤¤à¥à¤¯à¤¾à¤µà¤¶à¥à¤¯à¤• à¤µà¤¾à¤•à¥à¤¯à¤¾à¤‚à¤¶ à¤•à¥‹ à¤•à¤­à¥€ à¤¨ à¤¬à¤¦à¤²à¥‡à¤‚ - à¤œà¥ˆà¤¸à¤¾ à¤¦à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¹à¥ˆ à¤µà¥ˆà¤¸à¤¾ à¤¹à¥€ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚à¥¤\n5. à¤†à¤—à¥‡ à¤•à¥à¤¯à¤¾ à¤•à¤°à¥‡à¤‚ à¤®à¥‡à¤‚ à¤¹à¤®à¥‡à¤¶à¤¾ à¤¦à¥€ à¤—à¤ˆ à¤µà¤¿à¤¶à¤¿à¤·à¥à¤Ÿ à¤¸à¥à¤µà¤¿à¤§à¤¾ à¤•à¤¾ à¤¨à¤¾à¤® à¤¬à¤¤à¤¾à¤à¤‚à¥¤\n6. à¤ªà¥‚à¤°à¥‡ à¤¸à¤®à¤¯ à¤¸à¤°à¤², à¤¬à¥‹à¤²à¥€ à¤œà¤¾à¤¨à¥‡ à¤µà¤¾à¤²à¥€ à¤­à¤¾à¤·à¤¾ à¤•à¤¾ à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚à¥¤\n7. à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤…à¤¨à¥à¤­à¤¾à¤— à¤‰à¤ªà¤¸à¥à¤¥à¤¿à¤¤ à¤”à¤° à¤ªà¥‚à¤°à¥à¤£ à¤¹à¥‹à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤à¥¤\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤¸à¥‚à¤šà¤¨à¤¾\nà¤¯à¤¹ à¤•à¥‡à¤µà¤² à¤¸à¤¾à¤®à¥à¤¦à¤¾à¤¯à¤¿à¤• à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤•à¤¾à¤°à¥à¤¯à¤•à¤°à¥à¤¤à¤¾à¤“à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¤¹à¤¾à¤¯à¤• à¤®à¤¾à¤°à¥à¤—à¤¦à¤°à¥à¤¶à¤¨ à¤¹à¥ˆà¥¤\nà¤¯à¤¹ à¤¨à¥ˆà¤¦à¤¾à¤¨à¤¿à¤• â€‹â€‹à¤¸à¤²à¤¾à¤¹ à¤¯à¤¾ à¤šà¤¿à¤•à¤¿à¤¤à¥à¤¸à¥€à¤¯ à¤ªà¤°à¥€à¤•à¥à¤·à¤£ à¤•à¤¾ à¤¸à¥à¤¥à¤¾à¤¨ à¤¨à¤¹à¥€à¤‚ à¤²à¥‡à¤¤à¤¾à¥¤\nà¤¹à¤®à¥‡à¤¶à¤¾ à¤•à¤¿à¤¸à¥€ à¤¯à¥‹à¤—à¥à¤¯ à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤¦à¥‡à¤–à¤­à¤¾à¤² à¤ªà¥‡à¤¶à¥‡à¤µà¤° à¤¸à¥‡ à¤¸à¤‚à¤ªà¤°à¥à¤• à¤•à¤°à¥‡à¤‚à¥¤\nà¤†à¤ªà¤¾à¤¤à¤•à¤¾à¤²à¥€à¤¨ à¤à¤®à¥à¤¬à¥à¤²à¥‡à¤‚à¤¸: 108 à¤ªà¤° à¤•à¥‰à¤² à¤•à¤°à¥‡à¤‚ (à¤¨à¤¿à¤ƒà¤¶à¥à¤²à¥à¤•, 24/7)\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n Review needed â€” phrase detected: 'diagnos'\n\n\n======================================================================\nScenario 3 â€” Chest Pain & Breathlessness (Immediate escalation, English output)\n======================================================================\n [1/8] Text input â€” STT skipped.\n[2/8] Confirmation layer...\n [3/8] Translating to English (IndicTrans2)...\n   âœ“ An elderly man around 65 years old is having chest pain and difficulty breathing...\n[4/8] Retrieving Indian healthcare regulations (RAG)...\n   âœ“ Retrieved 284 words of regulatory context.\n [5/8] No image â€” skipped.\n[6/8] Running MedGemma ReAct agent (RAG-grounded)...\n   â†³ Pre-executing all 4 tools...\n   âœ“ get_first_aid_steps\n   âœ“ check_urgency\n   âœ“ lookup_nearest_facility\n   âœ“ generate_referral_note\n   â†³ MedGemma synthesising guidance pack...\n   âœ“ Guidance pack generated (189 words).\n   âœ“ Agent completed in 5 reasoning steps.\n [7/8] Applying safety guardrails...\n   IMMEDIATE ESCALATION TRIGGERED.\n [8/8] Translating output to English...\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nHELPER GUIDANCE PACK\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nWHAT TO SAY TO THE PATIENT OR FAMILY:\nHello, I am here to help. I understand you are having chest pain and difficulty breathing. I will help you get to a doctor as quickly as possible.\n\nFIRST AID â€” DO THIS RIGHT NOW:\n1.  Position the patient comfortably, sitting or leaning slightly forward.\n2.  Encourage the patient to breathe slowly and deeply.\n3.  Reassure the patient and explain what you are doing.\n\nWHAT NOT TO DO:\n1.  Do not give the patient anything to eat or drink.\n2.  Do not leave the patient alone.\n\nWHAT TO DO NEXT:\nCall 108 immediately â€” do not wait.\n\nURGENCY: This person needs to reach a doctor or hospital RIGHT NOW. Please call 108 immediately â€” do not wait.\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nSTRICT RULES:\n1. NEVER diagnose any condition.\n2. NEVER suggest medications, tablets, or injections.\n3. NEVER provide clinical treatment steps.\n4. NEVER change the urgency phrase â€” use it exactly as given.\n5. ALWAYS name the specific facility provided in WHAT TO DO NEXT.\n6. Use SIMPLE, SPOKEN language throughout.\n7. Every section must be present and complete.\n\nIMMEDIATE ACTION REQUIRED\nSigns of a serious emergency have been detected.\nCall 108 immediately â€” this is a free national ambulance service.\nDo not wait. Take the person to the nearest hospital now.\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n IMPORTANT NOTICE\nThis is assistive guidance for community health workers only.\nIt does NOT replace clinical advice or medical examination.\nAlways escalate to a qualified healthcare professional.\nEmergency Ambulance: Call 108 (free, 24/7)\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n Review needed â€” phrase detected: 'diagnos'\n\nAll demo scenarios completed.\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"# Gradio Interactive UI\n\nimport gradio as gr\n\nLANG_LIST = list(SUPPORTED_LANGUAGES.keys())\n\n\ndef gradio_transcribe(audio_path: str, input_language: str) -> tuple[str, str]:\n    \"\"\"Step 1: Transcribe audio and return for helper confirmation.\"\"\"\n    if not audio_path:\n        return \"\", \"No audio provided.\"\n    try:\n        lang_code            = SUPPORTED_LANGUAGES.get(input_language, \"hi\")\n        transcript, detected = transcribe_audio(audio_path, lang_code)\n        status = (\n            f\"Transcription complete (detected: {detected}).\\n\"\n            \"Please review the text below and correct any errors before proceeding.\"\n        )\n        return transcript, status\n    except Exception as e:\n        return \"\", f\" Transcription error: {str(e)}\"\n\n\ndef sync_output_language(input_lang: str) -> gr.update:\n    \"\"\"Auto-sync output language dropdown to match input language.\"\"\"\n    return gr.update(value=input_lang)\n\n\ndef gradio_generate(\n    audio_path      : str | None,\n    confirmed_text  : str,\n    text_direct     : str,\n    image_path      : str | None,\n    helper_notes    : str,\n    input_language  : str,\n    output_language : str,\n    region          : str,\n) -> tuple[str, str, str, str]:\n    \"\"\"Full pipeline handler. Returns (input_summary, image_analysis, reasoning_trace, guidance_pack).\"\"\"\n    try:\n        # â”€â”€ Resolve text input â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        if audio_path and confirmed_text.strip():\n            text_for_pipe = confirmed_text.strip()\n        elif text_direct.strip():\n            text_for_pipe = text_direct.strip()\n        else:\n            return \"Please provide audio or type notes before generating.\", \"\", \"\", \"\"\n\n        # â”€â”€ Handle image â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        image = None\n        if image_path:\n            image = Image.open(image_path).convert(\"RGB\")\n\n        # â”€â”€ Run pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        result = run_full_pipeline(\n            text_input      = text_for_pipe,\n            confirmed_text  = text_for_pipe,\n            image           = image,\n            helper_notes    = helper_notes,\n            input_language  = input_language,\n            output_language = output_language,\n            region          = region,\n            verbose         = False,\n        )\n\n        # â”€â”€ Input summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        input_summary = (\n            f\"Input mode     : {'Audio (transcribed)' if audio_path else 'Text'}\\n\"\n            f\"Language       : {input_language} â†’ {output_language}\\n\"\n            f\"Region         : {region}\\n\"\n            f\"RAG docs used  : {len(result['rag_context'].split())} words of Indian regulatory context\\n\\n\"\n            f\"Text used      :\\n{result['transcript_confirmed']}\"\n        )\n\n        # â”€â”€ Image analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        img_info = \"No image uploaded.\"\n        if result.get(\"image_context\"):\n            ctx      = result[\"image_context\"]\n            top_alts = sorted(ctx[\"all_labels\"].items(), key=lambda x: -x[1])[:4]\n            img_info = (\n                f\"Detected type  : {ctx['image_type']}\\n\"\n                f\"Confidence     : {ctx['confidence']:.0%}\\n\\n\"\n                f\"Top alternatives:\\n\"\n                + \"\\n\".join(f\"  â€¢ {k}: {v:.0%}\" for k, v in top_alts)\n            )\n\n        # â”€â”€ Reasoning trace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        trace = \"MedGemma Structured Agent â€” Tool Execution Trace:\\n\" + \"â”€\" * 40 + \"\\n\"\n        for step in result.get(\"reasoning_trace\", []):\n            trace += f\"\\nStep {step['step']}:\\n\"\n            if step.get(\"tool_called\"):\n                trace += f\"  â†’ Tool called : {step['tool_called']}\\n\"\n                if step.get(\"tool_result\"):\n                    result_preview = str(step['tool_result'])[:200]\n                    trace += f\"  â†’ Result      : {result_preview}...\\n\"\n            else:\n                trace += f\"{step['agent_output'][:300]}\\n\"\n\n        # â”€â”€ Final guidance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        guidance = result[\"guidance_pack_local\"]\n        if result.get(\"safety_warnings\"):\n            guidance += \"\\n\\n\" + \"\\n\".join(result[\"safety_warnings\"])\n\n        return input_summary, img_info, trace, guidance\n\n    except Exception as e:\n        import traceback\n        err = f\" Error: {str(e)}\\n{traceback.format_exc()}\"\n        return err, \"\", \"\", \"\"\n\n\n# â”€â”€ Build Gradio UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nwith gr.Blocks(\n    title = \" Community Helper Guidebook\",\n    theme = gr.themes.Soft(),\n) as demo:\n\n    gr.Markdown(\"\"\"\n    #  Community Helper Guidebook\n    ### Offline, Multimodal Healthcare Assistant for Low-Resource India\n    *Grounded in MoHFW / NHM / National Health Policy 2017*\n    ---\n    > **Assistive guidance only.** Does NOT replace clinical advice.\n    > Always escalate to a qualified healthcare professional. Emergency: **108**\n\n    **For:** ASHA Workers Â· Community Health Workers Â· NGO Volunteers Â· First-contact helpers\n    **Not for:** Patients directly Â· Clinical diagnosis Â· Treatment decisions\n    ---\n    \"\"\")\n\n    # â”€â”€ Language + Region â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    with gr.Row():\n        input_lang = gr.Dropdown(\n            choices = LANG_LIST,\n            value   = \"Hindi\",\n            label   = \"Helper's Language\",\n        )\n        output_lang = gr.Dropdown(\n            choices = LANG_LIST,\n            value   = \"Hindi\",\n            label   = \"Output Language (defaults to input language)\",\n        )\n        region_box = gr.Textbox(\n            value       = \"India\",\n            label       = \"Region / State\",\n            placeholder = \"e.g. Maharashtra, Rajasthan, Bihar\",\n        )\n\n    # Sync output language when input changes\n    input_lang.change(\n        fn      = sync_output_language,\n        inputs  = input_lang,\n        outputs = output_lang,\n    )\n\n    # â”€â”€ Input tabs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    with gr.Tabs():\n\n        with gr.TabItem(\" Voice Note\"):\n            gr.Markdown(\n                \"Upload a voice note in any format, or **record directly** in the browser.\"\n            )\n            audio_input = gr.Audio(\n                label   = \"Voice Note\",\n                sources = [\"upload\", \"microphone\"],\n                type    = \"filepath\",\n            )\n            transcribe_btn    = gr.Button(\"Transcribe Audio\", variant=\"secondary\")\n            transcribe_status = gr.Textbox(\n                label       = \"Transcription Status\",\n                interactive = False,\n                lines       = 2,\n            )\n            confirmed_text = gr.Textbox(\n                label       = \"Review & Correct Transcription\",\n                lines       = 4,\n                placeholder = \"Transcription appears here after clicking Transcribe. Correct any errors.\",\n            )\n            transcribe_btn.click(\n                fn      = gradio_transcribe,\n                inputs  = [audio_input, input_lang],\n                outputs = [confirmed_text, transcribe_status],\n            )\n\n        with gr.TabItem(\"Type Notes\"):\n            gr.Markdown(\"Type the helper's observation directly.\")\n            text_direct = gr.Textbox(\n                label       = \"Helper's Notes\",\n                lines       = 5,\n                placeholder = \"e.g. Baccha teen din se bukhaar mein hai...\",\n            )\n\n        with gr.TabItem(\"Medical Image\"):\n            gr.Markdown(\"Upload a photo if available (optional).\")\n            image_input = gr.Image(\n                label  = \"Medical Image (wound, rash, X-ray, prescription, etc.)\",\n                type   = \"filepath\",\n            )\n\n    # â”€â”€ Helper notes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    helper_notes_box = gr.Textbox(\n        label       = \"Helper's Additional Observations\",\n        lines       = 2,\n        placeholder = (\n            \"e.g. Child looks very weak. Family 40km from hospital. \"\n            \"No vehicle available. Family has Ayushman Bharat card.\"\n        ),\n    )\n\n    # â”€â”€ Generate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    generate_btn = gr.Button(\n        \"Generate Helper Guidance Pack\",\n        variant = \"primary\",\n        size    = \"lg\",\n    )\n\n    # â”€â”€ Outputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    gr.Markdown(\"---\\n### Results\")\n\n    with gr.Row():\n        with gr.Column(scale=1):\n            input_summary_out = gr.Textbox(\n                label       = \"Input Summary\",\n                lines       = 8,\n                interactive = False,\n            )\n            image_out = gr.Textbox(\n                label       = \"Image Analysis (SigLIP)\",\n                lines       = 8,\n                interactive = False,\n            )\n            trace_out = gr.Textbox(\n                label       = \"Agent Tool Execution Trace\",\n                lines       = 12,\n                interactive = False,\n            )\n\n        with gr.Column(scale=1):\n            guidance_out = gr.Textbox(\n                label       = \"Helper Guidance Pack\",\n                lines       = 30,\n                interactive = False,\n            )\n\n    generate_btn.click(\n        fn      = gradio_generate,\n        inputs  = [\n            audio_input, confirmed_text, text_direct,\n            image_input, helper_notes_box,\n            input_lang, output_lang, region_box,\n        ],\n        outputs = [input_summary_out, image_out, trace_out, guidance_out],\n    )\n\n    # â”€â”€ Demo examples â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    gr.Markdown(\"---\\n### Quick Demo Examples (click to load)\")\n    gr.Examples(\n        examples = [\n            [\n                None, \"\",\n                \"Baccha teen din se bukhaar mein hai. Woh kuch kha nahi raha.\",\n                None,\n                \"Child looks very weak. 40km from hospital. No vehicle.\",\n                \"Hindi\", \"Hindi\", \"Rajasthan\",\n            ],\n            [\n                None, \"\",\n                \"The woman's hand got burned while cooking. Redness and blistering on the palm.\",\n                None,\n                \"Mustard oil was applied. No doctor nearby. Family has Ayushman Bharat card.\",\n                \"English\", \"Hindi\", \"Maharashtra\",\n            ],\n            [\n                None, \"\",\n                \"Elderly man having chest pain and difficulty breathing. Left arm hurting. Sweating heavily.\",\n                None,\n                \"108 called, 45 minutes away. No oxygen available.\",\n                \"English\", \"English\", \"Uttar Pradesh\",\n            ],\n        ],\n        inputs = [\n            audio_input, confirmed_text, text_direct,\n            image_input, helper_notes_box,\n            input_lang, output_lang, region_box,\n        ],\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T16:09:19.165433Z","iopub.execute_input":"2026-02-24T16:09:19.166092Z","iopub.status.idle":"2026-02-24T16:09:28.225580Z","shell.execute_reply.started":"2026-02-24T16:09:19.166063Z","shell.execute_reply":"2026-02-24T16:09:28.224838Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# â”€â”€ Launch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\nLaunching Gradio...\")\ndemo.launch(share=True, debug=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T16:09:46.068369Z","iopub.execute_input":"2026-02-24T16:09:46.068873Z","iopub.status.idle":"2026-02-24T16:09:48.167586Z","shell.execute_reply.started":"2026-02-24T16:09:46.068844Z","shell.execute_reply":"2026-02-24T16:09:48.167044Z"}},"outputs":[{"name":"stdout","text":"\nLaunching Gradio...\n* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://981a1d49861e7ac1a9.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://981a1d49861e7ac1a9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stdout","text":"   â†³ Pre-executing all 4 tools...\n   âœ“ get_first_aid_steps\n   âœ“ check_urgency\n   âœ“ lookup_nearest_facility\n   âœ“ generate_referral_note\n   â†³ MedGemma synthesising guidance pack...\n   âœ“ Guidance pack generated (176 words).\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}